{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RZqfpwEgJBfz"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","!pip install netCDF4\n","#make sure to give accsess to the drive"]},{"cell_type":"markdown","source":["In this section, we are defining all function and needed imports"],"metadata":{"id":"YoSSUwngGS1K"}},{"cell_type":"code","source":["# @title imports and functions\n","import pandas as pd\n","import numpy as np\n","import scipy as sp\n","import netCDF4\n","from netCDF4 import Dataset\n","import geopy.distance\n","import io\n","from google.colab import files\n","import ast\n","import re\n","import glob\n","import matplotlib.pyplot as plt\n","import pickle\n","import math\n","from geopy.distance import geodesic\n","\n","import pyproj\n","import pandas as pd\n","from google.colab import drive\n","import folium\n","import pyproj\n","from math import radians, cos, sqrt\n","\n","\n","p_sweden = ( 57.70368 ,\t11.99507)\n","\n","min_val = 10\n","bucket_size = 5\n","M = 8\n","frq_intervals = [(min_val + i * bucket_size, min_val + (i + 1) * bucket_size) for i in range(M)]\n","colors = [\n","    '#0000FF',  # Blue\n","    '#006400',  # Dark Green\n","    '#8B0000',  # Dark Red\n","    '#008B8B',  # Dark Cyan\n","    '#2F4F4F',  # Dark Slate Gray\n","    '#556B2F',  # Dark Olive Green\n","    '#8B4513',  # Saddle Brown\n","    '#5E2605',  # Dark Brown\n","    '#483D8B',  # Dark Slate Blue (Not too purple)\n","    '#2E8B57'   # Sea Green\n","]\n","\n","def display_map(latitude, longitude):\n","    # Create a map centered at the given coordinates\n","    m = folium.Map(location=[latitude, longitude], zoom_start=13 , width='50%', height='50%')\n","    # Add a marker at the given coordinates\n","    folium.Marker([latitude, longitude]).add_to(m)\n","    # Display the map\n","    return m\n","\n","def display_map_with_line(lat1, lon1, lat2, lon2):\n","    # Create a map centered at the midpoint of the given coordinates\n","    m = folium.Map( location = [ ( lat1 + lat2 ) / 2, (lon1 + lon2) / 2], zoom_start=13)\n","    # Add markers at the given coordinates\n","    folium.Marker([lat1, lon1]).add_to(m)\n","    folium.Marker([lat2, lon2]).add_to(m)\n","    # Draw a line between the coordinates\n","    folium.PolyLine([(lat1, lon1), (lat2, lon2)], color=\"blue\").add_to(m)\n","    # Display the map\n","    return m\n","\n","\n","def add_point_to_map(m, latitude, longitude, tooltip_text = 'Point'):\n","    \"\"\"\n","    Add a point to an existing folium map.\n","\n","    Args:\n","        m (folium.Map): The existing map to which the point will be added.\n","        latitude (float): The latitude of the point.\n","        longitude (float): The longitude of the point.\n","        tooltip_text (str, optional): The tooltip text when hovering over the point. Defaults to 'Point'.\n","\n","    Returns:\n","        None: The function modifies the existing map in place.\n","    \"\"\"\n","    # Create a tooltip with custom HTML to set the text size\n","    tooltip = folium.Tooltip(f'<span style=\"font-size: 18px;\">{tooltip_text}</span>')\n","\n","    # Add a marker at the given coordinates with the tooltip\n","    folium.Marker(\n","          [latitude, longitude],\n","          tooltip=tooltip,\n","          icon=folium.Icon(color='red', icon='circle', prefix='fa')  # 'fa' stands for Font Awesome\n","      ).add_to(m)\n","\n","def add_line_to_map(m, row , txt='Try Text'):\n","    sublink = row['Sublink']\n","    lat1, lon1 = (row['NearLatitude_DecDeg'], row['NearLongitude_DecDeg']) # (x,y)\n","    lat2, lon2 = (row['FarLatitude_DecDeg'], row['FarLongitude_DecDeg'])\n","    frq =   row['Frequency_GHz']\n","\n","    # Create popups with coordinates\n","    popup1 = folium.Popup(f\"Coordinates: {lat1}, {lon1}\", parse_html=True)\n","    popup2 = folium.Popup(f\"Coordinates: {lat2}, {lon2}\", parse_html=True)\n","\n","    # Add CircleMarkers with popups\n","    folium.CircleMarker([lat1, lon1], radius=4, color='black', popup=popup1).add_to(m)\n","    folium.CircleMarker([lat2, lon2], radius=4, color='black', popup=popup2).add_to(m)\n","\n","    # sublink = row['Sublink']\n","    # lat1, lon1 = (row['NearLatitude_DecDeg'], row['NearLongitude_DecDeg']) # (x,y)\n","    # lat2, lon2 = (row['FarLatitude_DecDeg'], row['FarLongitude_DecDeg'])\n","    # frq =   row['Frequency_GHz']\n","    # small_icon = folium.Icon(icon_size=(10, 10))\n","    # #####\n","    # folium.CircleMarker([lat1, lon1], radius=4, color='black').add_to(m)\n","    # folium.CircleMarker([lat2, lon2], radius=4, color='black').add_to(m)\n","\n","    ###\n","    # Midpoint of the line\n","    mid_lat, mid_lon = (lat1 + lat2) / 2, (lon1 + lon2) / 2\n","\n","    # Tooltip with custom HTML to set the text size\n","\n","    bucket = into_bucket(frq , frq_intervals)\n","    txt = str(bucket)\n","    txt = str(sublink) + ' , ' + txt\n","    tooltip = folium.Tooltip(f'<span style=\"font-size: 18px;\">{txt}</span>')\n","\n","    # Create the polyline with the tooltip\n","    line = folium.PolyLine(\n","        [(lat1, lon1), (lat2, lon2)],\n","        color= colors[bucket] ,\n","        tooltip=tooltip\n","    )\n","\n","    # Add the line to the map\n","    line.add_to(m)\n","\n","\n","################################################################################\n","\n","def display_map_with_line(lat1, lon1, lat2, lon2):\n","    # Create a map centered at the midpoint of the given coordinates\n","    m = folium.Map( location = [ ( lat1 + lat2 ) / 2, (lon1 + lon2) / 2], zoom_start=13)\n","    # Add markers at the given coordinates\n","    folium.Marker([lat1, lon1]).add_to(m)\n","    folium.Marker([lat2, lon2]).add_to(m)\n","    # Draw a line between the coordinates\n","    folium.PolyLine([(lat1, lon1), (lat2, lon2)], color=\"blue\").add_to(m)\n","    # Display the map\n","    return m\n","\n","\n","def create_map(  p_center = p_sweden ):\n","    latitude, longitude = p_center\n","\n","    return folium.Map(location=[latitude, longitude], zoom_start=13)\n","\n","# @title map functions\n","from geopy.distance import geodesic\n","\n","\n","def create_map_sweden(df , m_map = None):\n","  if m_map == None:\n","    m_map = create_map()\n","  for index, row in df.iterrows():\n","    sublink = row['Sublink']\n","    add_line_to_map(m_map ,row)\n","  return m_map\n","\n","\n","def calculate_distance(point1, point2):\n","    \"\"\"\n","    Calculate the distance between two points on the Earth's surface\n","    using the geopy package.\n","\n","    Args:\n","        point1 (tuple): Latitude and longitude of point 1 as a tuple (lat, lon).\n","        point2 (tuple): Latitude and longitude of point 2 as a tuple (lat, lon).\n","\n","    Returns:\n","        float: Distance between the two points in kilometers.\n","    \"\"\"\n","    distance = geodesic(point1, point2).kilometers\n","    return distance\n","\n","def line_to_points(start, end, num_points):\n","    \"\"\"\n","    Splits the line into num_points and returns a list of these points.\n","    Args:\n","        start (tuple): Latitude and longitude of the start point (lat, lon).\n","        end (tuple): Latitude and longitude of the end point (lat, lon).\n","        num_points (int): Number of points to split the line into.\n","\n","    Returns:\n","        list: List of tuples representing points (lat, lon).\n","    \"\"\"\n","    points = []\n","    for i in range(num_points):\n","        t = i / float(num_points - 1)\n","        lat = (1 - t) * start[0] + t * end[0]\n","        lon = (1 - t) * start[1] + t * end[1]\n","        points.append((lat, lon))\n","    return points\n","\n","from geopy.distance import geodesic\n","\n","def calculate_distance(point1, point2):\n","    \"\"\"\n","    Calculate the distance between two points on the Earth's surface\n","    using the geopy package.\n","\n","    Args:\n","        point1 (tuple): Latitude and longitude of point 1 as a tuple (lat, lon).\n","        point2 (tuple): Latitude and longitude of point 2 as a tuple (lat, lon).\n","\n","    Returns:\n","        float: Distance between the two points in kilometers.\n","    \"\"\"\n","    distance = geodesic(point1, point2).kilometers\n","    return distance\n","\n","def average_distance_point_to_line(point, line):\n","    \"\"\"\n","    Calculate the average distance from a point to points along a line.\n","\n","    Args:\n","        point (tuple): Latitude and longitude of the target point as a tuple (lat, lon).\n","        line (list): List of tuples, each representing the latitude and longitude of a point on the line.\n","\n","    Returns:\n","        float: Average distance in kilometers from the point to the points on the line.\n","    \"\"\"\n","    total_distance = 0\n","    num_points = len(line)\n","\n","    for line_point in line:\n","        distance = calculate_distance(line_point, point)\n","        total_distance += distance\n","\n","    average_distance = total_distance / num_points\n","    return average_distance\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import plotly.express as px\n","\n","def plot_time_series(df, time_col, y_cols=None, interval=None, time_format='day', y_label=\"Data\", filter=False, legend_label='Data Signals', color='tab:blue'):\n","    \"\"\"\n","    Plot a time series data, which can be multivariate.\n","\n","    Args:\n","        df (DataFrame): DataFrame for the plot.\n","        time_col (str): Time column in df.\n","        y_cols (list): List of columns to plot.\n","        interval (tuple): Start and end datetime for filtering.\n","        time_format (str): Format of the time axis ('day' or 'hour:min').\n","        y_label (str): Label for the y-axis.\n","        filter (bool): Apply causal moving average filter if True.\n","    \"\"\"\n","    # Convert the time column to datetime\n","    df[time_col] = pd.to_datetime(df[time_col])\n","\n","    # Filter dataframe based on the interval\n","    if interval:\n","        start_date, end_date = interval\n","        df = filter_dataframe(df, time_col, start_date, end_date)\n","\n","    # Create the figure\n","    fig, ax = plt.subplots(figsize=(8, 6))\n","\n","    # Plotting for the time series\n","    if not y_cols:\n","        y_cols = [col for col in df.columns if col != time_col]\n","\n","    for col in y_cols:\n","        y = df[col]\n","        if filter:\n","            y = causal_moving_average(y, window_size=3)\n","        ax.plot(df[time_col], y, label=col, color=color)\n","\n","    ax.grid(True, alpha=0.3)\n","    ax.set_ylabel(y_label, fontsize=12)\n","    ax.tick_params(axis='both', which='major', labelsize=12)\n","    ax.set_xlabel('Time', fontsize=12)\n","\n","    # Format the x-axis dates\n","    locator = mdates.AutoDateLocator()\n","    if time_format == 'hour:min':\n","        formatter = mdates.DateFormatter('%H:%M')\n","    else:\n","        formatter = mdates.AutoDateFormatter(locator)\n","    ax.xaxis.set_major_locator(locator)\n","    ax.xaxis.set_major_formatter(formatter)\n","\n","    # Custom legend\n","    custom_legend = mlines.Line2D([], [], color=color, label=legend_label)\n","    ax.legend(handles=[custom_legend])\n","\n","    # Automatically adjust the x-axis labels and layout\n","    fig.autofmt_xdate()\n","    plt.tight_layout()\n","\n","    # Show the plot\n","    plt.show()\n","\n","def plot_2d_histogram_matplotlib(x, y, x_bins=30, y_bins=30):\n","    plt.hist2d(x, y, bins=(x_bins, y_bins), cmap=plt.cm.jet)\n","    plt.colorbar()\n","    plt.xlabel('X-axis')\n","    plt.ylabel('Y-axis')\n","    plt.title('2D Histogram with Matplotlib')\n","    plt.show()\n","\n","\n","def plot_hexbin_seaborn(x, y):\n","    sns.jointplot(x=x, y=y, kind='hex')\n","    plt.show()\n","\n","# Sample usage\n","\n","\n","def plot_2d_histogram_plotly(x, y, x_bins=30, y_bins=30):\n","    df = pd.DataFrame({'x': x, 'y': y})\n","    fig = px.density_heatmap(df, x='x', y='y', nbinsx=x_bins, nbinsy=y_bins, color_continuous_scale='Jet')\n","    fig.show()\n","\n","def plot_3d_scatter_matplotlib(x, y, z):\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    ax.scatter(x, y, z)\n","\n","    ax.set_xlabel('X-axis')\n","    ax.set_ylabel('Y-axis')\n","    ax.set_zlabel('Z-axis')\n","    plt.title('3D Scatter Plot with Matplotlib')\n","\n","    plt.show()\n","\n","# Sample usage\n","\n","\n","def plot_color_labels_row(colors, labels):\n","    \"\"\"\n","    Plots colored markers with corresponding text labels in a vertical layout.\n","\n","    Parameters:\n","        colors (list): List of color names.\n","        labels (list): List of labels for each color.\n","    \"\"\"\n","    if len(colors) != len(labels):\n","        print(\"Length of colors and labels lists must be the same.\")\n","        return\n","\n","    plt.figure(figsize=(2, 4))  # Reduced width to minimize horizontal space\n","\n","    for i, (color, label) in enumerate(zip(colors, labels)):\n","        plt.scatter(0, i, s=200, c=color)\n","        plt.text(0.1, i, label, ha='left', va='center', fontsize=14)  # Increased text size\n","\n","    plt.axis('off')\n","    plt.gca().invert_yaxis()  # Invert y-axis for top-to-bottom legend\n","    plt.tight_layout()  # Minimizes extra space within the plot\n","    plt.show()\n","\n","# Example usage\n","# colors = [\"darkgray\", \"navy\", \"teal\", \"maroon\", \"darkorange\", \"crimson\", \"red\"]\n","import matplotlib.pyplot as plt\n","\n","def plot_color_labels(colors, labels):\n","    \"\"\"\n","    Plots colored markers with corresponding text labels in a 2-column layout.\n","\n","    Parameters:\n","        colors (list): List of color names.\n","        labels (list): List of labels for each color.\n","    \"\"\"\n","    if len(colors) != len(labels):\n","        print(\"Length of colors and labels lists must be the same.\")\n","        return\n","\n","    plt.figure(figsize=(6, 4))  # Adjusted figure size to accommodate two columns\n","\n","    for i, (color, label) in enumerate(zip(colors, labels)):\n","        row = i // 2  # Determine which row to plot on\n","        col = i % 2   # Determine which column to plot on\n","\n","        x = col * 0.3  # x-position (0 or 0.5 to create two columns)\n","        y = row  # y-position\n","\n","        plt.scatter(x, y, s=200, c=color)\n","        # plt.text(x + 0.1, y, label, ha='left', va='center', fontsize=14)  # Increased text size\n","        plt.text(x + 0.03, y, label, ha='left', va='center', fontsize=24)\n","    plt.axis('off')\n","    plt.gca().invert_yaxis()  # Invert y-axis for top-to-bottom legend\n","    plt.tight_layout()  # Minimizes extra space within the plot\n","    plt.show()\n","\n","def plot_color_labels(colors, labels):\n","    \"\"\"\n","    Plots colored markers with corresponding text labels in a 2-column layout.\n","\n","    Parameters:\n","        colors (list): List of color names.\n","        labels (list): List of labels for each color.\n","    \"\"\"\n","    if len(colors) != len(labels):\n","        print(\"Length of colors and labels lists must be the same.\")\n","        return\n","\n","    plt.figure(figsize=(6, 4))  # Adjusted figure size for better visibility\n","\n","    for i, (color, label) in enumerate(zip(colors, labels)):\n","        row = i // 2  # Determine which row to plot on\n","        col = i % 2   # Determine which column to plot on\n","\n","        x = col * 0.02  # Adjusted x-position to decrease horizontal space\n","        y = row * 0.0002  # Adjusted y-position to decrease vertical space\n","\n","        plt.scatter(x, y, s=500, c=color)  # Increased marker size\n","        plt.text(x + 0.001, y, label, ha='left', va='center', fontsize=20)  # Increased text size\n","\n","    plt.axis('off')\n","    plt.gca().invert_yaxis()  # Invert y-axis for top-to-bottom legend\n","    plt.tight_layout(pad=0)  # Minimized extra space within the plot with pad=0\n","    plt.show()\n","\n","def plt_pred(label , pred , error = 1):\n","  plt.plot(pred , label = 'rg')\n","  plt.plot(label , label = 'link PL')\n","\n","  plt.legend()\n","  plt.show()\n","  if error:\n","    error = np.array(label) - np.array(pred)\n","    rmse = np.sqrt(np.mean((error)**2))\n","    plt.plot(error ,  label = f'rmse = {rmse}')\n","    plt.legend()\n","\n","\n","\n","def plot_side_by_side(data1, data2, titles = ['' , ''], x_labels ='', y_labels=''):\n","    \"\"\"\n","    Plots two datasets side by side.\n","\n","    Args:\n","    data1, data2 (tuple): Tuples containing x and y data points, e.g., (x1, y1) and (x2, y2)\n","    titles (list): Titles for the two plots.\n","    x_labels, y_labels (list): Labels for the x and y axes.\n","    \"\"\"\n","    fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # You can adjust the figure size\n","\n","    # Plotting the first dataset\n","    axes[0].plot(data1[0], data1[1])\n","    axes[0].set_title(titles[0])\n","    axes[0].set_xlabel(x_labels[0])\n","    axes[0].set_ylabel(y_labels[0])\n","    axes[0].grid(True)  # Adding grid\n","\n","    # Plotting the second dataset\n","    axes[1].plot(data2[0], data2[1])\n","    axes[1].set_title(titles[1])\n","    axes[1].set_xlabel(x_labels[1])\n","    axes[1].set_ylabel(y_labels[1])\n","    axes[1].grid(True)  # Adding grid\n","\n","    plt.tight_layout()  # Adjust layout to prevent overlap\n","    plt.show()\n","\n","def r_power_law(att_ts , a, b , L ):\n","  # calculate rain rate from attenuation\n","  r_t =  (att_ts / (a*L))**(1/b)\n","  return r_t\n","\n","def apply_r_power_law(df1, df2, col1, col2, a, b, L):\n","    df1[f'{col1}_r_power_law'] = r_power_law(df1[col1], a, b, L)\n","    df2[f'{col2}_r_power_law'] = r_power_law(df2[col2], a, b, L)\n","    return df1, df2\n","\n","def rain_event_detector_n( raw_gauges ,   min_event_time = 10 , stop_index = 5):\n","  '''\n","  We created a dictionary that contains for each rain gauge its indexes for\n","  rain intervals. We defined rain event as an event that lasts at least\n","  10 minutes and there is 15 minutes gap of dry indexes for each gauge\n","  '''\n","  #the minutes after the rain stop that need be recognaized as wet (for rain guages)\n","  RG_rain_indexes = {}\n","  for gauge in raw_gauges.columns[1:]:\n","    RG_rain_indexes[gauge] = []\n","  #adds all the gauge names to the array\n","  RG_Names = list(raw_gauges.columns[1:])\n","\n","  for gauge_name in RG_rain_indexes.keys():\n","    start_now=False # detect rain flag\n","    dry_days_count=0\n","    current_gauge_data = raw_gauges[gauge_name]\n","    data_size = len(current_gauge_data)\n","    for index in range(data_size):\n","      # means there is rain\n","      if current_gauge_data[index] > 0:\n","        dry_days_count = 0\n","        if start_now ==False:    #saves the start time of the rain event\n","          start_time =  index\n","          start_now = True\n","      #means that there is no rain\n","      elif dry_days_count < stop_index and start_now==True:  # non-rain\n","      # block to end rain event\n","        dry_days_count += 1\n","      #needs to insert the dates into the dictionary\n","      if dry_days_count==stop_index:\n","          #saves the end time of the rain event\n","            end_time = index\n","            event_tuple = (start_time,end_time)\n","            start_now=False\n","            dry_days_count=0\n","            #insert the dates into the dictionary\n","            RG_rain_indexes[gauge_name].append(event_tuple)\n","\n","  ########## filter rain event #######\n","  filter_rain  = stop_index + min_event_time\n","  for gauge_name in RG_rain_indexes.keys():\n","    # print(RG_rain_indexes[])\n","    event_list = RG_rain_indexes[gauge_name]\n","    for event in event_list:\n","      # length of rain event\n","      rain_len = event[1] - event[0]\n","      if rain_len < filter_rain:\n","        RG_rain_indexes[gauge_name].remove(event)\n","\n","\n","\n","  #lengths after filter rain event\n","  # print(\"----Number of rain events for each gauge---\")\n","  # for key, value in RG_rain_indexes.items():\n","    # print(len(RG_rain_indexes[key]))\n","  return RG_rain_indexes\n","\n","\n","def a_b(f_GHz, pol, approx_type=\"ITU\"):\n","    \"\"\"Approximation of parameters for A-R relationship\n","\n","    Parameters\n","    ----------\n","    f_GHz : int, float or np.array of these\n","            Frequency of the microwave link in GHz\n","    pol : str\n","            Polarization of the microwave link\n","    approx_type : str, optional\n","            Approximation type (the default is 'ITU', which implies parameter\n","            approximation using a table recommanded by ITU)\n","\n","    Returns\n","    -------\n","    a,b : float\n","          Parameters of A-R relationship\n","\n","    Note\n","    ----\n","    The frequency value must be between 1 Ghz and 100 GHz.\n","\n","    The polarization has to be indicated by 'h' or 'H' for horizontal and\n","    'v' or 'V' for vertical polarization respectively.\n","\n","    Currently only 'ITU' for approx_type is accepted. The approximation makes\n","    use of a table recommanded by ITU [4]_.\n","\n","    References\n","    ----------\n","    .. [4] ITU, \"ITU-R: Specific attenuation model for rain for use in\n","        prediction methods\", International Telecommunication Union, 2013\n","\n","    \"\"\"\n","    from scipy.interpolate import interp1d\n","\n","    f_GHz = np.asarray(f_GHz)\n","\n","    if f_GHz.min() < 1 or f_GHz.max() > 100:\n","        raise ValueError(\"Frequency must be between 1 Ghz and 100 GHz.\")\n","    else:\n","        if pol == \"V\" or pol == \"v\":\n","            f_a = interp1d(ITU_table[0, :], ITU_table[2, :], kind=\"cubic\")\n","            f_b = interp1d(ITU_table[0, :], ITU_table[4, :], kind=\"cubic\")\n","        elif pol == \"H\" or pol == \"h\":\n","            f_a = interp1d(ITU_table[0, :], ITU_table[1, :], kind=\"cubic\")\n","            f_b = interp1d(ITU_table[0, :], ITU_table[3, :], kind=\"cubic\")\n","        else:\n","            raise ValueError(\"Polarization must be V, v, H or h.\")\n","        a = f_a(f_GHz)\n","        b = f_b(f_GHz)\n","    return a, b\n","\n","from typing import List, Dict, Tuple\n","import pandas as pd\n","\n","def filter_events(events: List[Tuple[int, int]], min_duration: int) -> List[Tuple[int, int]]:\n","    \"\"\"Filter out events shorter than the minimum duration.\n","\n","    Args:\n","        events: List of start and end indices of detected events.\n","        min_duration: Minimum duration required for a valid event.\n","\n","    Returns:\n","        Filtered list of events.\n","    \"\"\"\n","    return [event for event in events if event[1] - event[0] >= min_duration]\n","\n","def identify_rain_events(gauge_data: pd.Series, stop_index: int ) -> List[Tuple[int, int]]:\n","    \"\"\"Identify rain events for a single gauge.\n","\n","    Args:\n","        gauge_data: The time series data for a single rain gauge.\n","        stop_index: Number of dry intervals to stop a rain event.\n","\n","    Returns:\n","        List of start and end indices of detected rain events.\n","    \"\"\"\n","    events = []\n","    start_event = None  # Variable to hold the starting index of a rain event\n","    dry_count = 0  # Counter for dry intervals\n","\n","    for index, value in enumerate(gauge_data):\n","        # Detect rain\n","        if value > 0:\n","            dry_count = 0  # Reset dry interval counter\n","            if start_event is None:\n","                start_event = index  # Mark the start of a rain event\n","\n","        # Detect dry interval within a rain event\n","        elif dry_count < stop_index and start_event is not None:\n","            dry_count += 1  # Increment dry interval counter\n","\n","        # End of rain event\n","        if dry_count == stop_index:\n","            end_event = index  # Mark the end of a rain event\n","            events.append((start_event, end_event))\n","            start_event = None  # Reset start index for the next event\n","            dry_count = 0  # Reset dry interval counter\n","\n","    return events\n","\n","def rain_event_detector(raw_gauges: pd.DataFrame, min_event_time: int = 10, stop_index: int = 5) -> Dict[str, List[Tuple[int, int]]]:\n","    \"\"\"Identify and filter rain events from raw gauge data.\n","\n","    Args:\n","        raw_gauges: DataFrame containing the raw gauge data.\n","        min_event_time: Minimum duration (in time steps) for a valid rain event.\n","        stop_index: Number of dry time steps required to stop a rain event.\n","\n","    Returns:\n","        Dictionary with gauge names as keys and lists of (start, end) indices as values.\n","    \"\"\"\n","    rain_events = {}  # Initialize the dictionary to hold rain events for all gauges\n","\n","    # Loop through each rain gauge to identify rain events\n","    for gauge_name in raw_gauges.columns[1:]:\n","        gauge_data = raw_gauges[gauge_name]\n","        events = identify_rain_events(gauge_data, stop_index)\n","\n","        # Filter out events that are shorter than the minimum event time\n","        filtered_events = filter_events(events, min_event_time + stop_index)\n","\n","        # Save the filtered events for this gauge\n","        rain_events[gauge_name] = filtered_events\n","\n","    return rain_events\n","\n","\n","def transform_raw_gauges(raw_gauges, W=10):\n","    \"\"\"\n","    Applies transformations on the raw_gauges DataFrame and returns a new DataFrame with fewer samples.\n","\n","    Parameters:\n","        raw_gauges: DataFrame containing the original raw gauge data.\n","        W: Window size for the averaging operation.\n","\n","    Returns:\n","        transformed_gauges: DataFrame containing the transformed data with the correct number of samples.\n","    \"\"\"\n","    # Initialize an empty dictionary to hold the transformed data temporarily\n","    transformed_dict = {}\n","\n","    # Calculate the length of the transformed series after averaging\n","    new_length = len(raw_gauges) // W\n","\n","    # Loop over each gauge and apply the transformations\n","    for gauge_name in raw_gauges.columns[1:]:\n","        y_rg   = interpolate_nan(raw_gauges[gauge_name].values)\n","        r_rg_h = rain_mm_to_h(y_rg)\n","        r_rg_5 = avg_ts(r_rg_h, W= W )\n","\n","        # Store the transformed series in the dictionary\n","        transformed_dict[gauge_name] = r_rg_5[:new_length]\n","\n","    # Create the transformed DataFrame\n","    transformed_gauges = pd.DataFrame(transformed_dict)\n","\n","    # Adjust the 'Time_UTC' to match the length of the transformed series\n","    transformed_gauges['Time_UTC'] = raw_gauges['Time_UTC'].iloc[::W].reset_index(drop=True).iloc[:new_length]\n","\n","    # Reorder the DataFrame columns to have 'Time_UTC' as the first column\n","    cols = ['Time_UTC'] + [col for col in transformed_gauges.columns if col != 'Time_UTC']\n","    transformed_gauges = transformed_gauges[cols]\n","    return transformed_gauges\n","\n","def a_b(f_GHz, pol, approx_type=\"ITU\"):\n","    \"\"\"Approximation of parameters for A-R relationship\n","\n","    Parameters\n","    ----------\n","    f_GHz : int, float or np.array of these\n","            Frequency of the microwave link in GHz\n","    pol : str\n","            Polarization of the microwave link\n","    approx_type : str, optional\n","            Approximation type (the default is 'ITU', which implies parameter\n","            approximation using a table recommanded by ITU)\n","\n","    Returns\n","    -------\n","    a,b : float\n","          Parameters of A-R relationship\n","\n","    Note\n","    ----\n","    The frequency value must be between 1 Ghz and 100 GHz.\n","\n","    The polarization has to be indicated by 'h' or 'H' for horizontal and\n","    'v' or 'V' for vertical polarization respectively.\n","\n","    Currently only 'ITU' for approx_type is accepted. The approximation makes\n","    use of a table recommanded by ITU [4]_.\n","\n","    References\n","    ----------\n","    .. [4] ITU, \"ITU-R: Specific attenuation model for rain for use in\n","        prediction methods\", International Telecommunication Union, 2013\n","\n","    \"\"\"\n","    from scipy.interpolate import interp1d\n","\n","    f_GHz = np.asarray(f_GHz)\n","\n","    if f_GHz.min() < 1 or f_GHz.max() > 100:\n","        raise ValueError(\"Frequency must be between 1 Ghz and 100 GHz.\")\n","    else:\n","        if pol == \"V\" or pol == \"v\":\n","            f_a = interp1d(ITU_table[0, :], ITU_table[2, :], kind=\"cubic\")\n","            f_b = interp1d(ITU_table[0, :], ITU_table[4, :], kind=\"cubic\")\n","        elif pol == \"H\" or pol == \"h\":\n","            f_a = interp1d(ITU_table[0, :], ITU_table[1, :], kind=\"cubic\")\n","            f_b = interp1d(ITU_table[0, :], ITU_table[3, :], kind=\"cubic\")\n","        else:\n","            raise ValueError(\"Polarization must be V, v, H or h.\")\n","        a = f_a(f_GHz)\n","        b = f_b(f_GHz)\n","    return a, b\n","# fmt: off\n","ITU_table = np.array(\n","    [\n","        [1.000e0,  2.000e0,  4.000e0,  6.000e0,  7.000e0,  8.000e0,  1.000e1,  1.200e1,  1.500e1,  2.000e1,  2.500e1,  3.000e1,  3.500e1,  4.000e1,  4.500e1,  5.000e1,  6.000e1,  7.000e1,  8.000e1,  9.000e1,  1.000e2],\n","        [3.870e-5, 2.000e-4, 6.000e-4, 1.800e-3, 3.000e-3, 4.500e-3, 1.010e-2, 1.880e-2, 3.670e-2, 7.510e-2, 1.240e-1, 1.870e-1, 2.630e-1, 3.500e-1, 4.420e-1, 5.360e-1, 7.070e-1, 8.510e-1, 9.750e-1, 1.060e0,  1.120e0],\n","        [3.520e-5, 1.000e-4, 6.000e-4, 1.600e-3, 2.600e-3, 4.000e-3, 8.900e-3, 1.680e-2, 3.350e-2, 6.910e-2, 1.130e-1, 1.670e-1, 2.330e-1, 3.100e-1, 3.930e-1, 4.790e-1, 6.420e-1, 7.840e-1, 9.060e-1, 9.990e-1, 1.060e0],\n","        [9.120e-1, 9.630e-1, 1.121e0,  1.308e0,  1.332e0,  1.327e0,  1.276e0,  1.217e0,  1.154e0,  1.099e0,  1.061e0,  1.021e0,  9.790e-1, 9.390e-1, 9.030e-1, 8.730e-1, 8.260e-1, 7.930e-1, 7.690e-1, 7.530e-1, 7.430e-1],\n","        [8.800e-1, 9.230e-1, 1.075e0,  1.265e0,  1.312e0,  1.310e0,  1.264e0,  1.200e0,  1.128e0,  1.065e0,  1.030e0,  1.000e0,  9.630e-1, 9.290e-1, 8.970e-1, 8.680e-1, 8.240e-1, 7.930e-1, 7.690e-1, 7.540e-1, 7.440e-1],\n","    ]\n",")\n","\n","def create_df(df , sublinks, str_key = 0):\n","  df_part = pd.DataFrame(columns = df.columns)\n","  for index, row in df.iterrows():\n","    sublink = row['Sublink']\n","    if str_key:\n","      sublink = str(sublink)\n","    if sublink not in sublinks:\n","        continue\n","    df_part = df_part.append(row , ignore_index= True)\n","  return df_part\n","import pandas as pd\n","\n","def create_df(df, sublinks, convert_sublink_to_str=False):\n","    # Convert sublinks to string if required\n","    if convert_sublink_to_str:\n","        df['Sublink'] = df['Sublink'].astype(str)\n","\n","    # Filter the DataFrame based on the sublinks\n","    filtered_df = df[df['Sublink'].isin(sublinks)]\n","\n","    return filtered_df\n","\n","\n","def get_row(df, key, col_key='Sublink', par='all'):\n","    \"\"\"\n","    Get the parameters of a site according to the site name.\n","\n","    Args:\n","        df (pd.DataFrame): A DataFrame containing all the devices associated with a site.\n","        key (str): The name to search for in the DataFrame.\n","        col_key (str, optional): The column name in the DataFrame where the key is located. Defaults to 'Sublink'.\n","        par (str, optional): The parameter to retrieve from the site's row. Use 'all' to return the entire row. Defaults to 'all'.\n","\n","    Returns:\n","        pd.Series or any: The entire row or the specified parameter if found, None otherwise.\n","    \"\"\"\n","    for index, row in df.iterrows():\n","        if str(key) in str(row[col_key]):\n","            if par == 'all':\n","                return row\n","            else:\n","                return row[par]\n","    return None\n","\n","def calculate_distance_P_line(lat1_near, long1_near, lat2_far, long2_far, lat_gauge, long_gauge):\n","    # Convert all latitudes and longitudes to radians\n","    lat1_near_rad = math.radians(lat1_near)\n","    long1_near_rad = math.radians(long1_near)\n","    lat2_far_rad = math.radians(lat2_far)\n","    long2_far_rad = math.radians(long2_far)\n","    lat_gauge_rad = math.radians(lat_gauge)\n","    long_gauge_rad = math.radians(long_gauge)\n","\n","    # Calculate the x and y differences in longitudes\n","    delta_x = (long2_far_rad - long1_near_rad) * math.cos((lat1_near_rad + lat2_far_rad) / 2)\n","    delta_y = lat2_far_rad - lat1_near_rad\n","\n","    # Calculate the distance using the Pythagorean theorem\n","    distance = math.sqrt(delta_x**2 + delta_y**2)\n","\n","    return distance\n","\n","def caculate_mid(point_1 , point_2): # point: (x,y)\n","  x_mid = (np.array(point_1) + np.array((point_2)))/2\n","  return x_mid\n","\n","\n","def calculate_distance(point1, point2):\n","    \"\"\"\n","    Calculate the distance between two points on the Earth's surface\n","    using the geopy package.\n","\n","    Args:\n","        point1 (tuple): Latitude and longitude of point 1 as a tuple (lat, lon).\n","        point2 (tuple): Latitude and longitude of point 2 as a tuple (lat, lon).\n","\n","    Returns:\n","        float: Distance between the two points in kilometers.\n","    \"\"\"\n","    distance = geodesic(point1, point2).kilometers\n","    return distance\n","\n","def flat_earth_distance(p1, p2):\n","    # Convert latitude and longitude from degrees to radians\n","    lat1, lon1 = map(radians, p1)\n","    lat2, lon2 = map(radians, p2)\n","\n","    # Constants for converting degrees to kilometers\n","    RADIUS_OF_EARTH_KM = 6371\n","    lat_diff_km = (lat2 - lat1) * RADIUS_OF_EARTH_KM\n","    lon_diff_km = (lon2 - lon1) * RADIUS_OF_EARTH_KM * cos((lat1 + lat2) / 2)\n","\n","    # Use Pythagoras' theorem to calculate the distance\n","    distance = sqrt(lat_diff_km**2 + lon_diff_km**2)\n","\n","    return distance\n","\n","\n","def close_sublinks (df,  gauge_name, distance = 1 ):\n","  CMLs_index= df.loc[pd_distance_RG_links[gauge_name] < distance].index.tolist()\n","  df_low_distance= df.loc[CMLs_index]\n","  sorted_df = df_low_distance.sort_values(by=gauge_name, ascending=True)\n","  # Get the indices for the first, middle, and last rows\n","  # Convert the \"Sublink\" column to integer type\n","  sorted_df[\"Sublink\"] = sorted_df [\"Sublink\"].astype(int)\n","  # Keep only the column specified by gauge_name\n","  sorted_df = sorted_df[[gauge_name, \"Sublink\" ]]\n","  sorted_df['distance'] = sorted_df[gauge_name]\n","  sorted_df = sorted_df.drop(gauge_name, axis=1)\n","  return sorted_df\n","\n","\n","def rain_mm_to_h (rain_mm , sample = 'min'):\n","    # Convert each value from mm/min to mm/h\n","    if sample == 'min':\n","      r_mm_h= np.array(rain_mm) * 60\n","    return r_mm_h\n","\n","def avg_ts(ts , W=5):\n","    # Calculate the average over each 5-minute interval\n","    avg_ts = []\n","    for i in range(0, len(ts) - (W-1), W):\n","        avg_w = np.mean(ts[i:i+W])\n","        avg_ts.append(avg_w)\n","    return avg_ts\n","\n","# @title buckets\n","# Function to create buckets and intervals from a list of values\n","import math\n","\n","def bucketize_list(input_list, M, round_option=False , b_size = None):\n","    min_val, max_val = min(input_list), max(input_list)\n","\n","    if round_option:\n","        # Round down the minimum value to the nearest multiple of 5\n","        min_val = math.floor(min_val / 5) * 5\n","\n","        # Round up the maximum value to the nearest multiple of 5\n","        max_val = math.ceil(max_val / 5) * 5\n","    if b_size != None:\n","      bucket_size = b_size\n","    else:\n","      bucket_size = (max_val - min_val) / M\n","    buckets = [[] for _ in range(M)]\n","    intervals = [(min_val + i * bucket_size, min_val + (i + 1) * bucket_size) for i in range(M)]\n","\n","    for val in input_list:\n","        bucket_index = int((val - min_val) // bucket_size)\n","        if bucket_index == M:\n","            bucket_index = M - 1\n","        buckets[bucket_index].append(val)\n","\n","    return buckets, intervals\n","\n","\n","\n","# Function to categorize a single new value into an existing bucket\n","def into_bucket(new_value, intervals):\n","    for i, (lower, upper) in enumerate(intervals):\n","        if lower <= new_value < upper:\n","            return i\n","        elif i == len(intervals) - 1 and new_value == upper:\n","            return i\n","    return None  # Return None if the value is out of range of existing intervals\n","\n","def lst_bucket(lst, intervals):\n","    bucket_lst = []\n","    for value in lst:\n","      bucket = into_bucket(value , intervals)\n","      bucket_lst.append(bucket)\n","    return bucket_lst\n","# Example usage\n","\n","\n","\n","# Example usage with a random time series in mm/min\n","# time_series_mm_per_min = raw_gauges[g_name][RG_rain_events[g_name][0][]]\n","  # Replace this with your actual time series\n","\n","\n","def downsample_ts(ts, window):\n","  ts = pd.Series(ts).rolling(window).mean().values[::90]\n","  return ts\n","\n","\n","def min_max_normalize(data):\n","    \"\"\"\n","    Applies min-max normalization to a given data array.\n","    Parameters:\n","        data (array-like): The input data to be normalized.\n","    Returns:\n","        normalized_data (array-like): The normalized data array.\n","    \"\"\"\n","    min_val = min(data)\n","    max_val = max(data)\n","    normalized_data = [(x - min_val) / (max_val - min_val) for x in data]\n","    return normalized_data\n","\n","\n","def min_normalize(data):\n","    \"\"\"\n","    min_norm\n","    \"\"\"\n","    min_val = min(data)\n","    normalized_data = data - min_val\n","    return normalized_data\n","\n","\n","def interpolate_nan(data):\n","  x_inputs = data.copy()\n","  mask = np.isnan(data)\n","  x_inputs[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), data[~mask])\n","  return x_inputs\n","\n","def replace_nan_with_previous(arr):\n","    \"\"\"\n","    Replace NaN values in a NumPy array with the previous non-NaN value.\n","    Args:\n","        arr (ndarray): Input NumPy array.\n","    Returns:\n","        ndarray: NumPy array with NaN values replaced by previous non-NaN values.\n","    \"\"\"\n","    mask = np.isnan(arr)\n","    indices = np.where(~mask, np.arange(mask.shape[0]), 0)\n","    np.maximum.accumulate(indices, axis=0, out=indices)\n","    return arr[indices]\n","\n","\n","import pandas as pd\n","\n","def compute_dataframe_differences(df1, df2):\n","    \"\"\"\n","    Computes the difference between two DataFrames and various summary statistics for the differences.\n","\n","    Args:\n","    df1, df2 (DataFrame): The DataFrames to compute the differences between.\n","\n","    Returns:\n","    dict: A dictionary containing the difference DataFrame and summary statistics.\n","    \"\"\"\n","    # Calculate the difference between the corresponding entries in the two DataFrames\n","    diff = df1 - df2\n","\n","    # Compute the average difference per column\n","    diff_avg = diff.mean()\n","\n","    # Compute the maximum difference per column\n","    diff_max = diff.max()\n","\n","    # Compute the minimum difference per column\n","    diff_min = diff.min()\n","\n","    # Compute the median of the differences per column\n","    diff_median = diff.median()\n","\n","    # Compute the standard deviation of the differences per column\n","    diff_std = diff.std()\n","\n","    # Compile all the computed statistics into a dictionary\n","    results = {\n","        \"difference\": diff,\n","        \"average\": diff_avg,\n","        \"max\": diff_max,\n","        \"min\": diff_min,\n","        \"median\": diff_median,\n","        \"std_deviation\": diff_std\n","    }\n","\n","    return results\n","\n","import datetime\n","def convert_unix_to_datetimes(unix_timestamps):\n","    \"\"\"\n","    Automatically detect the unit (milliseconds or seconds) of Unix timestamps\n","    and convert them to datetime objects.\n","\n","    Args:\n","        unix_timestamps (array-like): An array of Unix timestamps.\n","\n","    Returns:\n","        List of datetime objects.\n","    \"\"\"\n","    # Check if the timestamps are in seconds or milliseconds\n","    # Assuming any timestamp larger than 1e11 is in milliseconds\n","    if all(ts > 1e11 for ts in unix_timestamps):\n","        # Timestamps are in milliseconds\n","        return [datetime.datetime.fromtimestamp(ts / 1000) for ts in unix_timestamps]\n","    else:\n","        # Timestamps are in seconds\n","        return [datetime.datetime.fromtimestamp(ts) for ts in unix_timestamps]\n","\n","from sklearn.svm import OneClassSVM\n","from sklearn.cluster import KMeans\n","import warnings\n","from sklearn.cluster import KMeans\n","# Suppress FutureWarning for n_init\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","from sklearn.metrics import confusion_matrix\n","\n","def accuracy(y_true, y_pred):\n","    if len(y_true) != len(y_pred):\n","      print(\"Wrong: Both input lists must have the same length.\")\n","    correct = sum(y_t == y_p for y_t, y_p in zip(y_true, y_pred))\n","    return correct / len(y_true)\n","\n","def acc_confusion_matrix(y_true, y_pred):\n","    matrix = confusion_matrix(y_true, y_pred)\n","    return matrix\n","\n","def window_kmeans(y_base  ,  k = 3 ):\n","  #create a copy to y_base, so we wont ruin y_base\n","  y_base_copy = y_base.copy()\n","  # Calculate the number of elements to trim from the end of the array to make it divisible by 6 (we lost only the last 10 seconds from mesurment of 3 months - so its okay)\n","  remainder = len(y_base_copy) % 6\n","  if remainder > 0:\n","      y_base_copy = y_base_copy[:-remainder]\n","  #creates an array of arrays that each one contains 6 items (each array represents one minute)\n","  reshaped_y_base_copy = y_base_copy.reshape(-1, 6)\n","  # Calculate the mean and the std along axis 1 (mean of each six values)\n","  MA = np.mean(reshaped_y_base_copy, axis=1)\n","  STD= np.std(reshaped_y_base_copy, axis=1)\n","  #STEP 3: K-MEANS\n","  # Combine 'means' and 'std' arrays into a single feature array\n","  feature_array = np.column_stack((MA, STD))\n","  # Initialize the KMeans object with the chosen number of clusters\n","  kmeans = KMeans(n_clusters=k, random_state=42)\n","  # Fit the KMeans object to our feature array\n","  kmeans.fit(feature_array)\n","  # Get the cluster centers and labels\n","  cluster_centers = kmeans.cluster_centers_\n","  labels = kmeans.labels_\n","  if k == 3 :\n","    for i in range(len(labels)):\n","      if labels[i] == 2:\n","        labels[i] = 1\n","\n","  return labels\n","########\n","def one_class_svm_classification(time_series, nu=0.05):\n","    \"\"\"\n","    Perform binary classification on a time series using One-Class SVM.\n","    \"\"\"\n","    model = OneClassSVM(nu=nu, kernel=\"rbf\")\n","    model.fit(time_series.reshape(-1, 1))\n","    predictions = model.predict(time_series.reshape(-1, 1))\n","    return predictions\n","\n","def kmeans_pred_2d(feature_1, feature_2, c=2, plot_flag=False):\n","    # Create feature matrix\n","    feature_matrix = np.vstack([feature_1, feature_2]).T\n","\n","    # K-means clustering\n","    kmeans = KMeans(n_clusters=c)\n","    kmeans.fit(feature_matrix)\n","    labels = kmeans.labels_\n","    centers = kmeans.cluster_centers_\n","\n","    # Check if cluster `0` center has higher values and swap labels if needed\n","    if centers[0, :].sum() > centers[1, :].sum():\n","        labels = 1 - labels  # This swaps 0s for 1s and vice versa for a binary classification\n","\n","    if plot_flag:\n","        # Create 2D scatter plot using average and standard deviation\n","        plt.scatter(feature_matrix[:, 0], feature_matrix[:, 1], c=labels, cmap='viridis')\n","        # Plot cluster centers\n","        plt.scatter(centers[:, 0], centers[:, 1], c='black', marker='x', s=100, label='Cluster Centers')\n","        plt.xlabel('Feature 1')\n","        plt.ylabel('Feature 2')\n","        plt.title('2D Plot of Clusters')\n","        plt.legend()\n","        plt.show()\n","\n","    return labels\n","\n","\n","# test to see that both rain gauge start mesurment time and sublink mesurment time are the same time\n","from datetime import datetime\n","\n","def unix_to_readable(unix_time):\n","    return datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S')\n","\n","# Test the function\n","\n","def readable_to_unix(readable_time, time_format='%Y-%m-%d %H:%M:%S'):\n","    dt = datetime.strptime(readable_time, time_format)\n","    return int((dt - datetime(1970, 1, 1)).total_seconds())\n","\n","# Test the function\n","#unix_to_readable(1694997689)\n","#link_times = np.array(raw_links.variables['time'][6 : -1])\n","#readable_dates = list(map(unix_to_readable, link_times))\n","\n","def ma_with_resample(data, window_size):\n","    # Initialize an empty list to store the moving average values\n","    ma_values = []\n","\n","    # Loop through the data array\n","    for i in range(len(data) - window_size + 1):\n","        # Extract the window of samples\n","        window = data[i:i + window_size]\n","\n","        # Resample the window (in this example, we'll do a random shuffle)\n","        resampled_window = np.random.choice(window, size=window_size, replace=True)\n","\n","        # Calculate the average of the resampled window\n","        ma_value = np.mean(resampled_window)\n","\n","        # Append the moving average value to the list\n","        ma_values.append(ma_value)\n","\n","    return np.array(ma_values)\n","# Create some\n","from sklearn.cluster import KMeans\n","\n","def rmse_ts(time_series_1, time_series_2):\n","    if len(time_series_1) != len(time_series_2):\n","        raise ValueError(\"The two time series must have the same length.\")\n","    return np.sqrt(np.mean((np.array(time_series_1) - np.array(time_series_2))**2))\n","\n","# rain estimation:\n","def calculate_PL (sublink_num, y_ma):\n","  rain_estimation = []\n","  for value in y_ma:\n","    if value <= 0:\n","      PL = 0\n","    else:\n","      a_cml , b_cml = sublinks_ab[int(sublink_num)]\n","      L_cml = sublinks_L[int(sublink_num)]\n","      PL    = r_power_law( value , a_cml , b_cml, L_cml)\n","    # add array:\n","    rain_estimation.append(PL)\n","  return rain_estimation\n","\n","def moving_average_filter(data, W):\n","    return np.convolve(data, np.ones(W)/W, 'valid')\n","\n","def moving_std_filter(data, W):\n","    return [np.std(data[i:i+W]) for i in range(len(data) - W + 1)]\n","\n","\n","def std_ma_filter_2(data , W = 6):\n","  filtered_data = moving_average_filter(data, W)\n","  resampled_data  = filtered_data[: : W]\n","  std_data = moving_std_filter(data, W)\n","  resampled_std   = std_data[ : : W]\n","  return resampled_data , resampled_std\n","\n","\n","def std_ma_filter(data, W = 6):\n","\n","    # Ensure data is divisible by W by trimming any extra values\n","    length = len(data) // W * W\n","    data = data[ : length]\n","\n","    # Reshape data to group every 'W' consecutive data points\n","    reshaped_data = data.reshape(-1, W)\n","\n","    # Calculate the mean and std for each group\n","    MA  = np.mean(reshaped_data, axis=1)\n","    STD = np.std(reshaped_data, axis=1)\n","    return MA, STD\n","\n","\n","def rain_filter(data ,W = 1 , task = 'estimation'):\n","  # Ensure data is divisible by W by trimming any extra values\n","  length = len(data) // W * W\n","  data = data[ : length]\n","  # Reshape data to group every 'W' consecutive data points\n","  reshaped_data =   data.reshape(-1, W)\n","  data_filtered =   np.mean(reshaped_data, axis=1)\n","  if task == 'classification':\n","    return data_filtered > 0\n","  if task == 'estimation':\n","    return data_filtered\n"],"metadata":{"cellView":"form","id":"ijrpwA_GBGO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title read_files_folder_function\n","\n","\n","def read_files_folder(path, file_type='.json', print_non_matching=False, read_contents=False, return_type='list', include_subfolders=True):\n","    \"\"\"\n","    Return a list or dictionary of files or their contents in the given path (and optionally its subfolders) that end with the specified file type.\n","    Optionally print the names of the files that do not match the file type and read their contents.\n","\n","    Args:\n","        path (str): The path to search for files.\n","        file_type (str): The file extension to match. Default is 'json'.\n","        print_non_matching (bool): Flag to print non-matching file names. Default is False.\n","        read_contents (bool): Flag to read contents of the files. Default is False.\n","        return_type (str): The type of the return value ('list' or 'dict'). Default is 'list'.\n","        include_subfolders (bool): Flag to include subfolders in the search. Default is True.\n","\n","    Returns:\n","        matched_files (list or dict): List or dictionary of matched files or their contents with the specified file type.\n","    \"\"\"\n","    if return_type == 'dict':\n","        matched_files = {}\n","    else:\n","        matched_files = []\n","\n","    non_matching_files = []\n","\n","    for root, dirs, files in os.walk(path):\n","        for file in files:\n","            try:\n","              if not include_subfolders and root != path:\n","                  continue\n","\n","              file_path = os.path.join(root, file)\n","              if file.endswith(file_type):\n","                  content = read_file(file_path) if read_contents else file_path\n","\n","                  if return_type == 'dict':\n","                      matched_files[file] = content\n","                  else:\n","                      matched_files.append(content)\n","              else:\n","                  non_matching_files.append(file_path)\n","            except:\n","               print('File Failed:', file_path)\n","\n","    if print_non_matching and non_matching_files:\n","        print(f\"Non-matching files (not .{file_type}):\")\n","        for file in non_matching_files:\n","            print(file)\n","\n","    return matched_files\n","\n"],"metadata":{"id":"MRL8sptqSnBf","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UdWpeUZxI0NW","cellView":"form"},"outputs":[],"source":["# @title import data\n","import pandas as pd\n","import netCDF4\n","import os\n","meta_mrg_path_links     =    '/content/gdrive/My Drive/Wireless-Weather/Open_Datasets/OpenMRG_Sweeden/cml/cml_metadata/'\n","\n","data_files = read_files_folder(meta_mrg_path_links, '.csv')\n","\n","# Define main root path and dataset name\n","# main_root =     '/content/gdrive/My Drive/Wireless-Weather/Open_Datasets/'\n","main_root =     '/content/gdrive/My Drive/Wireless-Weather/Open_Datasets/'\n","sweden_dataset = 'OpenMRG_Sweden/'\n","\n","read_sweden = 1\n","# Define paths for metadata and raw data\n","if read_sweden:\n","  meta_mrg_path_links  = f\"{main_root}{sweden_dataset}cml/cml_metadata.csv\"\n","  raw_mrg_path_links   = f\"{main_root}{sweden_dataset}cml/cml.nc\"\n","  meta_mrg_path_gauges = f\"{main_root}{sweden_dataset}gauges/city/CityGauges-metadata.csv\"\n","  raw_mrg_path_gauges  = f\"{main_root}{sweden_dataset}gauges/city/CityGauges-2015JJA.csv\"\n","\n","# Flag to indicate whether to read all MRG data\n","read_all_mrg = True\n","\n","# Read metadata\n","meta_links = pd.read_csv(meta_mrg_path_links)\n","meta_gauges = pd.read_csv(meta_mrg_path_gauges)\n","\n","# Read raw data\n","raw_links = netCDF4.Dataset(raw_mrg_path_links)\n","#understanding the data\n","# print(\"\\nVariables:\")\n","# for var_name, var in raw_links.variables.items():\n","#     print(f\"Name: {var_name}\")\n","#     print(f\"Dimensions: {var.dimensions}\")\n","#     print(f\"Shape: {var.shape}\")\n","#     print(f\"Attributes:\")\n","#     for attr in var.ncattrs():\n","#         print(f\"  {attr}: {getattr(var, attr)}\")\n","#     print(f\"Data (first 10 elements): {var[:10]}\")  # Adjust to print a subset of data\n","#     print(\"\\n\")\n","\n","raw_gauges = pd.read_csv(raw_mrg_path_gauges)\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["# @title present gauge dataframes\n","# present data : raw_gauges , meta_gauges, meta_links\n","show_var = raw_gauges\n","print(\"Table of the gauges values as a function of time:\")\n","show_var"],"metadata":{"id":"FpqWccBt9AYz","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Plotting a Rain Gauge data\n","raw_gauges['Time']  = pd.to_datetime(raw_gauges['Time_UTC']).dt.strftime('%Y-%m-%d %H:%M:%S')\n","# choosing 2 rain gauges by names\n","RG_1 = 'Bergsj'\n","RG_2 = 'Torsl'\n","gauge_ts_1 = raw_gauges[RG_1]\n","gauge_ts_2 = raw_gauges[RG_2]\n","# Example usage with your specific data\n","RG_labels = ['Bergsj', 'Torsl']  # Assuming this comes from your dataset description\n","x_time =   list(range(len(gauge_ts_1)))  # Assuming gauge_1 and gauge_2 are indexed by time or similar\n","plot_side_by_side((x_time, gauge_ts_1), (x_time, gauge_ts_2), RG_labels, ['Time', 'Time'], ['Acc. Rain [mm]', 'Acc. Rain [mm]'])\n"],"metadata":{"id":"Zr4x7LzsJDO_","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Another functions\n","df= meta_gauges\n","# Extract the list of column names from meta_gauges\n","rg_lst = meta_gauges['Name']\n","\n","# Create an empty DataFrame with the specified columns\n","pd_distance_RG_links = pd.DataFrame(columns=rg_lst)\n","\n","# Extract the 'Sublink' rows from meta_links\n","rows = meta_links['Sublink']\n","\n","# Add the 'Sublink' rows to the new DataFrame\n","pd_distance_RG_links['Sublink'] = rows\n","\n","# Dictionary to store coordinates of each rain gauge\n","gauges_coordinates = {}\n","for index, row in df.iterrows():\n","    name = row['Name']\n","    latitude = row['Latitude_DecDeg']\n","    longitude = row['Longitude_DecDeg']\n","    gauges_coordinates[name] = (latitude, longitude)\n","\n","# List of rain gauge names for reference\n","rg_lst = list(gauges_coordinates.keys())\n","\n","# Display the DataFrame\n","#print(pd_distance_RG_links)\n","\n","# Save the middle coordinate for each sublink\n","sublinks_coordinates_mid = {}\n","\n","# Dictionary to store the full line coordinate for each sublink\n","sublinks_coordinates_line = {}\n","\n","# Dictionary to store unspecified coordinates for each sublink, initialized without description\n","sublinks_coordinates = {}\n","# sublinks_frq - Dictionary to store frequency data for each sublink\n","sublinks_frq = {}\n","\n","# sublinks_ab - Dictionary to store start and end points (a, b) for each sublink\n","sublinks_ab = {}\n","\n","# sublinks_L - Dictionary to store the length of each sublink\n","sublinks_L = {}\n","\n","for index, row in meta_links.iterrows():\n","  # Read only A direction of the sublink (even-indexed rows)\n","  sublink = row['Sublink']\n","  near_coordinates = (row['NearLatitude_DecDeg'], row['NearLongitude_DecDeg']) # (x,y)\n","  far_coordinates = (row['FarLatitude_DecDeg'], row['FarLongitude_DecDeg'])\n","  sublinks_coordinates[sublink] = [(near_coordinates)]\n","  sublinks_coordinates[sublink].append(far_coordinates)\n","  # mid coordinate of the sublink\n","  sublinks_coordinates_mid[sublink]  = caculate_mid(near_coordinates , far_coordinates)\n","  # line represented by points:\n","  sublinks_coordinates_line[sublink] = line_to_points(near_coordinates , far_coordinates , 5)\n","  # create frq dict\n","  sublinks_frq[sublink] = row['Frequency_GHz']\n","  # create frq dict\n","  # a: sublinks_ab[sublink][0] , b: sublinks_ab[sublink][1]\n","  sublinks_ab[sublink]  = a_b( row['Frequency_GHz'] , 'v')\n","  sublinks_L[sublink] =  row['Length_km']\n","\n","# print('---- The two coordinates for each sublink -----')\n","# print(sublinks_coordinates)\n","# print('---- The middle coordinate of each sublink ----')\n","# print(sublinks_coordinates_mid)\n","\n","# Load metadata for gauges\n","df = meta_gauges\n","\n","pd_distance_RG_lines = pd_distance_RG_links.copy()\n","\n","# Flag to trigger creation of new distance measurements\n","create_dist = 1\n","\n","if create_dist:\n","    # Dictionaries to store average and midpoint distances for each sublink\n","    dict_avg_dist = {}\n","    dict_mid_dist = {}\n","\n","    for gauge_name, gauge_loc in gauges_coordinates.items():\n","        lat_gauge, long_gauge = gauge_loc\n","\n","        # Prepare numpy array for distance calculations\n","        point = np.array((gauge_loc))\n","        dis_list = []\n","        dis_avg = []\n","\n","        for sublink, link_loc in sublinks_coordinates_mid.items():\n","            line_link = sublinks_coordinates_line[sublink]\n","            distance_mid = calculate_distance(point, link_loc)\n","            distance_avg = average_distance_point_to_line(point, line_link)\n","            value_avg = (gauge_name, distance_avg)\n","            value_mid = (gauge_name, distance_mid)\n","\n","            # Store distances in dictionaries\n","            if sublink not in dict_avg_dist:\n","                dict_avg_dist[sublink] = []\n","            if sublink not in dict_mid_dist:\n","                dict_mid_dist[sublink] = []\n","\n","            dict_avg_dist[sublink].append(value_avg)\n","            dict_mid_dist[sublink].append(value_mid)\n","\n","            # Append distances to lists for DataFrame updates\n","            dis_list.append(distance_mid)\n","            dis_avg.append(distance_avg)\n","\n","        # Update DataFrame with new distance lists\n","        pd_distance_RG_links[gauge_name] = dis_list\n","        pd_distance_RG_lines[gauge_name] = dis_avg\n","\n","## Check differences\n","results = compute_dataframe_differences(pd_distance_RG_lines, pd_distance_RG_links)\n","# print('G')\n","# print(pd_distance_RG_links)\n","\n","# Initialize lists to store data about sublinks:\n","\n","sublinks_lst = [] # part of links datasets\n","dis_lst = [] # list of Rg-Links Distances\n","frq_lst = []\n","lens_lst = []\n","ab_lst = []\n","\n","# List of gauge names to skip\n","skip_gauge = ['Name', 'Sublink', 'Askim', 'Barl']\n","\n","# Dictionary to store closest sublinks data for each gauge\n","df_closest_dict = {}\n","\n","# Iterate over columns in pd_distance_RG_links\n","for gauge_name in pd_distance_RG_links.columns:\n","    if gauge_name in skip_gauge:\n","        continue\n","    # Calculate closest sublinks\n","    value_df = close_sublinks(pd_distance_RG_lines, gauge_name, distance = 1)\n","    df_closest_dict[gauge_name] = value_df\n","\n","    # Extend lists with values from value_df\n","    dis_lst.extend(value_df['distance'])\n","    sublinks_lst.extend(value_df['Sublink'].values)\n","\n","    for sublink in value_df['Sublink'].values:\n","        frq_lst.append(sublinks_frq[sublink])\n","        lens_lst.append(sublinks_L[sublink])\n","        ab_lst.append(sublinks_ab[sublink])\n","\n","#print:\n","# df_closest_dict['Bergsj']\n","# df_closest_dict['Torp']"],"metadata":{"id":"qoyXFl8uIEdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title map functions\n","import pyproj\n","import pandas as pd\n","from google.colab import drive\n","import folium\n","import pyproj\n","from math import radians, cos, sqrt\n","\n","\n","p_sweden = ( 57.70368 ,\t11.99507)\n","\n","min_val = 10\n","bucket_size = 5\n","M = 8\n","frq_intervals = [(min_val + i * bucket_size, min_val + (i + 1) * bucket_size) for i in range(M)]\n","colors = [\n","    '#0000FF',  # Blue\n","    '#006400',  # Dark Green\n","    '#8B0000',  # Dark Red\n","    '#008B8B',  # Dark Cyan\n","    '#2F4F4F',  # Dark Slate Gray\n","    '#556B2F',  # Dark Olive Green\n","    '#8B4513',  # Saddle Brown\n","    '#5E2605',  # Dark Brown\n","    '#483D8B',  # Dark Slate Blue (Not too purple)\n","    '#2E8B57'   # Sea Green\n","]\n","\n","def display_map(latitude, longitude):\n","    # Create a map centered at the given coordinates\n","    m = folium.Map(location=[latitude, longitude], zoom_start=13 , width='50%', height='50%')\n","    # Add a marker at the given coordinates\n","    folium.Marker([latitude, longitude]).add_to(m)\n","    # Display the map\n","    return m\n","\n","def display_map_with_line(lat1, lon1, lat2, lon2):\n","    # Create a map centered at the midpoint of the given coordinates\n","    m = folium.Map( location = [ ( lat1 + lat2 ) / 2, (lon1 + lon2) / 2], zoom_start=13)\n","    # Add markers at the given coordinates\n","    folium.Marker([lat1, lon1]).add_to(m)\n","    folium.Marker([lat2, lon2]).add_to(m)\n","    # Draw a line between the coordinates\n","    folium.PolyLine([(lat1, lon1), (lat2, lon2)], color=\"blue\").add_to(m)\n","    # Display the map\n","    return m\n","\n","def add_point_to_map(m,row_gauge  , clr = 'red'):\n","    \"\"\"\n","    Add a point to an existing folium map.\n","\n","    Args:\n","        m (folium.Map): The existing map to which the point will be added.\n","        latitude (float): The latitude of the point.\n","        longitude (float): The longitude of the point.\n","        tooltip_text (str, optional): The tooltip text when hovering over the point. Defaults to 'Point'.\n","\n","    Returns:\n","        None: The function modifies the existing map in place.\n","    \"\"\"\n","\n","    lat_gauge, long_gauge = row_gauge['Latitude'], row_gauge['Longitude']\n","    gauge_name = row_gauge['Station Id']\n","    # Create a tooltip with custom HTML to set the text size\n","    # tooltip = folium.Tooltip(f'<span style=\"font-size: 18px;\">{gauge_name}</span>')\n","\n","    # Add a marker at the given coordinates with the tooltip\n","\n","    folium.Marker(\n","          [lat_gauge, long_gauge],\n","          tooltip=tooltip,\n","          icon=folium.Icon(color='red', icon='circle', prefix='fa')  # 'fa' stands for Font Awesome\n","      ).add_to(m)\n","\n","def add_point_to_map(m, latitude, longitude, tooltip_text = 'Point'):\n","    \"\"\"\n","    Add a point to an existing folium map.\n","\n","    Args:\n","        m (folium.Map): The existing map to which the point will be added.\n","        latitude (float): The latitude of the point.\n","        longitude (float): The longitude of the point.\n","        tooltip_text (str, optional): The tooltip text when hovering over the point. Defaults to 'Point'.\n","\n","    Returns:\n","        None: The function modifies the existing map in place.\n","    \"\"\"\n","    # Create a tooltip with custom HTML to set the text size\n","    tooltip = folium.Tooltip(f'<span style=\"font-size: 18px;\">{tooltip_text}</span>')\n","\n","    # Add a marker at the given coordinates with the tooltip\n","    folium.Marker(\n","          [latitude, longitude],\n","          tooltip=tooltip,\n","          icon=folium.Icon(color='red', icon='circle', prefix='fa')  # 'fa' stands for Font Awesome\n","      ).add_to(m)\n","\n","\n","def add_line_to_map(m, row , txt='Try Text'):\n","    sublink = row['Sublink']\n","    lat1, lon1 = (row['NearLatitude_DecDeg'], row['NearLongitude_DecDeg']) # (x,y)\n","    lat2, lon2 = (row['FarLatitude_DecDeg'], row['FarLongitude_DecDeg'])\n","    frq =   row['Frequency_GHz']\n","\n","    # Create popups with coordinates\n","    popup1 = folium.Popup(f\"Coordinates: {lat1}, {lon1}\", parse_html=True)\n","    popup2 = folium.Popup(f\"Coordinates: {lat2}, {lon2}\", parse_html=True)\n","\n","    # Add CircleMarkers with popups\n","    folium.CircleMarker([lat1, lon1], radius=4, color='black', popup=popup1).add_to(m)\n","    folium.CircleMarker([lat2, lon2], radius=4, color='black', popup=popup2).add_to(m)\n","    ###\n","    # Midpoint of the line\n","    mid_lat, mid_lon = (lat1 + lat2) / 2, (lon1 + lon2) / 2\n","\n","    # Tooltip with custom HTML to set the text size\n","\n","    bucket = into_bucket(frq , frq_intervals)\n","    # txt = str(bucket)\n","    txt_frq =  str(frq) + '(GHz)'\n","    txt = str(sublink) + ' , ' + txt_frq\n","    tooltip = folium.Tooltip(f'<span style=\"font-size: 18px;\">{txt}</span>')\n","\n","    # Create the polyline with the tooltip\n","    line = folium.PolyLine(\n","        [(lat1, lon1), (lat2, lon2)],\n","        color= colors[bucket] ,\n","        tooltip=tooltip\n","    )\n","\n","    # Add the line to the map\n","    line.add_to(m)\n","\n","\n","\n","\n","################################################################################\n","\n","def display_map_with_line(lat1, lon1, lat2, lon2):\n","    # Create a map centered at the midpoint of the given coordinates\n","    m = folium.Map( location = [ ( lat1 + lat2 ) / 2, (lon1 + lon2) / 2], zoom_start=13)\n","    # Add markers at the given coordinates\n","    folium.Marker([lat1, lon1]).add_to(m)\n","    folium.Marker([lat2, lon2]).add_to(m)\n","    # Draw a line between the coordinates\n","    folium.PolyLine([(lat1, lon1), (lat2, lon2)], color=\"blue\").add_to(m)\n","    # Display the map\n","    return m\n","\n","\n","def create_map(  p_center = p_sweden ):\n","    latitude, longitude = p_center\n","\n","    return folium.Map(location=[latitude, longitude], zoom_start=13)\n","\n","# @title map functions\n","from geopy.distance import geodesic\n","\n","\n","def create_map_sweden(df , m_map = None):\n","  if m_map == None:\n","    m_map = create_map()\n","  for index, row in df.iterrows():\n","    sublink = row['Sublink']\n","    add_line_to_map(m_map ,row)\n","  return m_map\n","\n","\n","def calculate_distance(point1, point2):\n","    \"\"\"\n","    Calculate the distance between two points on the Earth's surface\n","    using the geopy package.\n","\n","    Args:\n","        point1 (tuple): Latitude and longitude of point 1 as a tuple (lat, lon).\n","        point2 (tuple): Latitude and longitude of point 2 as a tuple (lat, lon).\n","\n","    Returns:\n","        float: Distance between the two points in kilometers.\n","    \"\"\"\n","    distance = geodesic(point1, point2).kilometers\n","    return distance\n","\n","def line_to_points(start, end, num_points):\n","    \"\"\"\n","    Splits the line into num_points and returns a list of these points.\n","\n","    Args:\n","        start (tuple): Latitude and longitude of the start point (lat, lon).\n","        end (tuple): Latitude and longitude of the end point (lat, lon).\n","        num_points (int): Number of points to split the line into.\n","\n","    Returns:\n","        list: List of tuples representing points (lat, lon).\n","    \"\"\"\n","    points = []\n","    for i in range(num_points):\n","        t = i / float(num_points - 1)\n","        lat = (1 - t) * start[0] + t * end[0]\n","        lon = (1 - t) * start[1] + t * end[1]\n","        points.append((lat, lon))\n","    return points\n","\n","from geopy.distance import geodesic\n","\n","def calculate_distance(point1, point2):\n","    \"\"\"\n","    Calculate the distance between two points on the Earth's surface\n","    using the geopy package.\n","\n","    Args:\n","        point1 (tuple): Latitude and longitude of point 1 as a tuple (lat, lon).\n","        point2 (tuple): Latitude and longitude of point 2 as a tuple (lat, lon).\n","\n","    Returns:\n","        float: Distance between the two points in kilometers.\n","    \"\"\"\n","    distance = geodesic(point1, point2).kilometers\n","    return distance\n","\n","def average_distance_point_to_line(point, line):\n","    \"\"\"\n","    Calculate the average distance from a point to points along a line.\n","\n","    Args:\n","        point (tuple): Latitude and longitude of the target point as a tuple (lat, lon).\n","        line (list): List of tuples, each representing the latitude and longitude of a point on the line.\n","\n","    Returns:\n","        float: Average distance in kilometers from the point to the points on the line.\n","    \"\"\"\n","    total_distance = 0\n","    num_points = len(line)\n","\n","    for line_point in line:\n","        distance = calculate_distance(line_point, point)\n","        total_distance += distance\n","\n","    average_distance = total_distance / num_points\n","    return average_distance\n","\n","\n","def create_df(df, sublinks, convert_sublink_to_str=False):\n","    # Convert sublinks to string if required\n","    if convert_sublink_to_str:\n","        df['Sublink'] = df['Sublink'].astype(str)\n","\n","    # Filter the DataFrame based on the sublinks\n","    filtered_df = df[df['Sublink'].isin(sublinks)]\n","\n","    return filtered_df\n","\n","\n","\n","\n","df_data   = create_df( meta_links,  sublinks_lst )\n","\n","# Colors from least intense to most intense\n","colors = [\"red\", \"darkred\", \"chocolate\", \"darkorange\", \"navy\", \"deepskyblue\", \"dodgerblue\", \"blue\"]\n","\n","# create map for all RGs and CMLs\n","\n","# lst_sublinks = y_filter.keys()\n","\n","frq_buckets , frq_intervals= bucketize_list(meta_links['Frequency_GHz'], 8 , round_option=True, b_size= 5)\n","\n","# remove sublinks from the map\n","link_to_remove = 560\n","try:\n","  sublinks_lst.remove(link_to_remove)\n","except:\n","  pass\n","\n","maps = []\n","\n","#####\n","all_data_flag   = 1\n","maps_flag       = 1\n","create_map_flag = 1\n","\n","\n","if all_data_flag:\n","  map_sweden  =   create_map_sweden(df_data)\n","##\n","  for gauge_name, gauge_loc in gauges_coordinates.items():\n","    if gauge_name in skip_gauge:\n","      continue\n","    lat_gauge, long_gauge = gauge_loc\n","    add_point_to_map( map_sweden , lat_gauge, long_gauge , tooltip_text = gauge_name)\n","##\n","if maps_flag:\n","  for gauge_name, gauge_loc in gauges_coordinates.items():\n","    if gauge_name in skip_gauge:\n","      continue\n","    sublinks_lst = df_closest_dict[gauge_name]['Sublink'].values\n","    df_t = create_df( meta_links, sublinks_lst )\n","    if create_map_flag:\n","      m = create_map_sweden(df_t)\n","    lat_gauge, long_gauge = gauge_loc\n","    add_point_to_map( m, lat_gauge, long_gauge , tooltip_text = gauge_name)\n","    maps.append(m)\n","\n","print(df_data.head())\n","print(len(df_data))"],"metadata":{"id":"vsmhf7yUbVnb","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title cluster gauges according to link's length\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","\n","# Sample DataFrame (replace with your actual df_data)\n","df_data = pd.DataFrame({'Length_km': [0.69144, 0.69144, 1.48256, 1.48256, 0.45866,\n","                                      1.06601, 1.07598, 1.07598, 1.41213, 1.41213]})\n","\n","# K-Means clustering\n","kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n","df_data['cluster'] = kmeans.fit_predict(df_data[['Length_km']])\n","\n","# Sort cluster centers\n","centers = np.sort(kmeans.cluster_centers_.flatten())\n","\n","# Find threshold values (midpoints between centers)\n","thresholds = [(centers[i] + centers[i+1]) / 2 for i in range(len(centers)-1)]\n","\n","# Plot the clusters\n","plt.figure(figsize=(8, 5))\n","scatter = plt.scatter(df_data.index, df_data['Length_km'], c=df_data['cluster'], cmap='viridis', edgecolors='k', s=100)\n","\n","# Plot cluster centers\n","plt.scatter(range(len(centers)), centers, color='red', marker='X', s=200, label='Cluster Centers')\n","\n","# Add threshold lines\n","for threshold in thresholds:\n","    plt.axhline(y=threshold, color='gray', linestyle='--', label=f'Threshold: {threshold:.5f}')\n","\n","# Labels & Title\n","plt.xlabel('Index')\n","plt.ylabel('Length_km')\n","plt.title('K-Means Clustering of Length_km')\n","plt.legend()\n","plt.show()\n","\n","# Display results\n","print(\"Cluster Centers:\", centers)\n","print(\"Thresholds (Boundaries):\", thresholds)\n","print(df_data)\n"],"metadata":{"id":"03h37rm24z_E","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Display Map\n","map_sweden"],"metadata":{"id":"51OlXTrDlv3s","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title creating/saving/loading the df_mts\n","import os\n","import pickle\n","import pandas as pd\n","import numpy as np\n","import datetime\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","chunk_size = 75\n","chunk_to_generate_save = 9  # Change this to the desired chunk index to generate between 0 to 728/chuncksize (upper value)\n","\n","\n","# Define basic paths\n","SAVE_PATH = f'/content/gdrive/My Drive/df_mts_chunk_{chunk_to_generate_save}.pkl'\n","#LOAD_PATH = '/content/gdrive/My Drive/df_mts_*.pkl'\n","LOAD_PATH = '/content/gdrive/My Drive/Wireless-Weather/df_mts_data/df_mts_*.pkl'\n","\n","# Define flags for creating, saving, and loading data\n","create_data, save_data, load_data = 0, 0, 1  # Set create_data to 0 to only save specific chunk\n","\n","# Define the sublinks index to use\n","sublinks_index = list(np.arange(0, 728))\n","\n","# Define the flag for attenuation\n","attenuation = True\n","\n","if create_data:\n","    times = raw_links.variables['time'][:]\n","    date_times = convert_unix_to_datetimes(times)\n","\n","\n","\n","    # Specify the chunk index to save (0-based index)\n","    chunk_to_generate = chunk_to_generate_save  # Change this to the desired chunk index to generate between 0 to 15\n","\n","\n","\n","\n","    df_mts = pd.DataFrame()\n","    df_mts['Time'] = date_times\n","\n","    start_idx = chunk_to_generate * chunk_size\n","    end_idx = min((chunk_to_generate + 1) * chunk_size, len(sublinks_index))\n","\n","    for sublink in sublinks_index[start_idx:end_idx]:\n","        df_raw = pd.DataFrame()\n","        df_raw['Time'] = date_times\n","        #print('reading sublink...', sublink)\n","\n","        # Load rx data and handle missing values (replace with actual data loading)\n","        rx = np.array(raw_links.variables[\"rsl\"][:, sublink])\n","        rx[rx == 1e10] = np.nan\n","        df_raw[f'rsl_{sublink}'] = rx\n","\n","        # If attenuation is true, load tx data and calculate attenuation (replace with actual data loading)\n","        if attenuation:\n","            tx = np.array(raw_links.variables[\"tsl\"][:, sublink])\n","            tx[tx == 1e10] = np.nan\n","            df_raw[f'tsl_{sublink}'] = tx\n","            df_raw[f'diff_{sublink}'] = tx - rx\n","\n","        df_mts[f'rsl_{sublink}'] = df_raw[f'rsl_{sublink}']\n","\n","    # Save the DataFrame chunk only if create_data is enabled\n","    # if save_data:\n","    #     # Ensure the directory exists before saving\n","    #     save_dir = os.path.dirname(SAVE_PATH)\n","    #     os.makedirs(save_dir, exist_ok=True)\n","\n","    #     chunk_path = SAVE_PATH.format(chunk_to_generate)\n","    #     df_mts.to_pickle(chunk_path)\n","    #     print(f'Saved chunk {chunk_to_generate} to {chunk_path}')\n","\n","#### Loading Data and Concatenating (replace with actual loading if needed)\n","if load_data:\n","    import glob\n","    all_files = glob.glob(LOAD_PATH)\n","    df_list = []\n","\n","    for file in all_files:\n","        df_list.append(pd.read_pickle(file))\n","        #print(f'Loaded {file}')\n","\n","    df_mts = pd.concat(df_list, axis=1)\n","    # print('Concatenated DataFrame:')\n","    # print(df_mts)\n","\n","\n","\n","\n","\n","\n","\n","\n","# import pickle\n","# import pandas as pd\n","# import numpy as np\n","# import datetime\n","\n","# # Define basic paths\n","# SAVE_PATH = 'path_to_save_file.pkl'\n","# LOAD_PATH = 'path_to_load_file.pkl'\n","\n","# # Define flags for creating, saving, and loading data\n","# create_data, save_data, load_data = 1, 0, 0\n","# ####\n","# # Define the sublinks index to use\n","# sublinks_index = [5, 1, 2]\n","\n","# # Define the flag for attenuation\n","# attenuation = True\n","\n","# if create_data:\n","#     times = raw_links.variables['time'][:]\n","#     date_times = convert_unix_to_datetimes(times)\n","\n","#     # Create an empty DataFrame with the time column\n","#     df_raw_dict = {}\n","#     df_mts = pd.DataFrame()\n","#     df_mts['Time'] = date_times\n","\n","#     print('creating data...')\n","\n","#     for sublink in sublinks_index:\n","#         df_raw = pd.DataFrame()\n","#         df_raw['Time'] = date_times\n","#         print('reading sublink...', sublink)\n","\n","#         # Load rx data and handle missing values\n","#         rx = np.array(raw_links.variables[\"rsl\"][:, sublink])\n","#         rx[rx == 1e10] = np.nan\n","#         df_raw[f'rsl_{sublink}'] = rx\n","\n","#         # If attenuation is true, load tx data and calculate attenuation\n","#         if attenuation:\n","#             tx = np.array(raw_links.variables[\"tsl\"][:, sublink])\n","#             tx[tx == 1e10] = np.nan\n","#             df_raw[f'tsl_{sublink}'] = tx\n","#             df_raw[f'diff_{sublink}'] = tx - rx\n","\n","#         # Add the DataFrame to the dictionary\n","#         df_raw_dict[sublink] = df_raw.copy()\n","#         df_mts[f'rsl_{sublink}'] = df_raw[f'rsl_{sublink}']\n","\n","\n","# if save_data:\n","#     # Save the dictionary to a file using pickle\n","#     with open(SAVE_PATH, 'wb') as f:\n","#         pickle.dump(df_raw_dict, f)\n","#         print('saved')\n","\n","# if load_data:\n","#     # Load the dictionary from the saved file using pickle\n","#     with open(LOAD_PATH, 'rb') as f:\n","#         df_raw_links = pickle.load(f)\n","\n","# Filter out duplicate Time names, keeping only the first occurrence\n","# print(\"Original columns:\")\n","# print(df_mts.columns)\n","columns_to_drop = []\n","found_first_time = False\n","arr = list(df_mts['Time'].iloc[:,0])\n","for i, col in enumerate(df_mts.columns):\n","    if col == \"Time\" and not found_first_time:\n","        found_first_time = True\n","    elif col == \"Time\" and found_first_time:\n","        columns_to_drop.append(i)\n","\n","# Drop columns by their indices\n","df_mts = df_mts.drop(df_mts.columns[columns_to_drop], axis=1)\n","df_mts.insert(loc=0, column='Time', value=arr)\n","# Print modified columns\n","# print(\"Modified columns:\")\n","# print(df_mts.columns)\n","# for i in range(len(df_mts.columns)):\n","#   print(df_mts.columns[i],i)\n","# print(\"df_mts:\")\n","# print(df_mts.head())"],"metadata":{"id":"Y99mbRQyhiUl","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title data selection from df_mts\n","list_sublinks = [337, 202, 568, 333, 15, 600, 552, 530, 164]\n","df_mts = df_mts.iloc[:,list_sublinks]\n","# print(df_mts)"],"metadata":{"id":"fUNWS6tfyOx0","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title subtruct median\n","def subtract_median(df):\n","    # Create a copy of the dataframe to avoid changing the original\n","    df_adjusted = df.copy()\n","\n","    # Iterate through each column in the dataframe\n","    for col in df.columns:\n","        if col != 'Time':\n","            # Subtract the median value from each column (except 'Time')\n","            median_value = df[col].median()\n","            df_adjusted[col] = abs(df[col] - median_value)\n","\n","    return df_adjusted\n","\n","df_rain_att =  subtract_median(df_mts)\n","# df_rain_att[10000:11100]\n","#print(df_rain_att)\n","df_rain_att\n","# print(max(list((df_rain_att.iloc[:,1]))))\n","\n","#print(df_rain_att['rsl_336'])\n"],"metadata":{"id":"GPFQm6_iy697","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title rain classification per sublink and graphs\n","def get_rain_classification_from_sub_link(df_rain_att,sub_link_number,treshold):\n","  sub_link_number_key = \"rsl_\" + str(sub_link_number)\n","  if sub_link_number_key not in df_rain_att.columns:\n","    print(f\"The sublink {sub_link_number} doesn't exist in the df_mts, include him in the 'data selection from df_mts' form\" )\n","    return 0, 0\n","  df_rain_att_clas = df_rain_att.copy()\n","  # print(df_rain_att)\n","\n","  # df_rain_att_clas.iloc[:-1,1:] = df_rain_att.iloc[:-1,1:].applymap(lambda x : 1 if x > treshold else 0)\n","  # print(\"df_rain_att_clas\")\n","  # print(df_rain_att_clas)\n","\n","  df_rain_att_clas.iloc[:-1,:] = df_rain_att.iloc[:-1,:].applymap(lambda x : 1 if x > treshold else 0)\n","  # print(\"df_rain_att_clas\")\n","  # print(df_rain_att_clas)\n","\n","\n","  #pick a sublinl number to see classifictation of rain\n","  sublink_num = str(sub_link_number)\n","  sublink_num = \"rsl_\"+ sublink_num\n","  # class_rain_per_time = df_rain_att_clas.loc[:,[\"Time\",sublink_num]]\n","  class_rain_per_time = df_rain_att_clas.loc[:,[sublink_num]]\n","  #print(class_rain_per_time[10000:11100])\n","  new_column_values = list(range(0, 7948870, 10))  # Generates [0, 10, 20, ..., 100]\n","  # Add this list as a new column to your DataFrame\n","  class_rain_per_time['time_binary'] = new_column_values\n","  #print(class_rain_per_time[10000:11100])\n","  class_rain_per_time\n","  #plot graph:\n","  # plt.figure(figsize=(100, 5))  # Adjust figure size as needed\n","  # plt.plot(class_rain_per_time['time_binary'], class_rain_per_time[sublink_num], marker='o', linestyle='-', color='b', label='rsl_5')\n","  # plt.title('Plot of rsl_5 vs. time_binary')\n","  # plt.xlabel('time_binary')\n","  # plt.ylabel('rsl_5')\n","  # plt.grid(True)\n","  # plt.legend()\n","  # plt.tight_layout()\n","\n","  # # Show plot\n","  # plt.show()\n","\n","\n","  #manage the time\n","  result = []\n","  # Iterate over the DataFrame in chunks of 6 rows\n","  for i in range(0, len(class_rain_per_time), 6):\n","      chunk = class_rain_per_time.iloc[i:i+6]  # Get the chunk of 6 rows\n","\n","      # Check if any value in 'rsl_164' column is greater than 0 in this chunk\n","      if (chunk[sublink_num].sum() > 5):\n","          result.append(1)  # Append 1 if condition is met\n","      else:\n","          result.append(0)  # Append 0 if condition is not met\n","  #print(result)\n","  #print((result[2:]))\n","  # Create a new DataFrame from the results\n","  # time_array = np.arange(0,len(class_rain_per_time)+1)\n","  # new_df = pd.DataFrame({'result_column': result,'time': time_array})\n","  return class_rain_per_time,result[2:]\n","\n","\n","class_rain_per_time336,results336_tresh_0 = get_rain_classification_from_sub_link(df_rain_att,529,0)\n","class_rain_per_time336,classification_sub_link_336_tresh_1 = get_rain_classification_from_sub_link(df_rain_att,529,1)\n","class_rain_per_time336,results336_tresh_5 = get_rain_classification_from_sub_link(df_rain_att,529,5)\n","# print(class_rain_per_time336[100:200])\n","# print(len(class_rain_per_time336))\n","# print(results336_tresh_0)\n","# print(  len(results336_tresh_0))"],"metadata":{"id":"2yJ61Xfv9qC9","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title get the d closest gauges from sublink\n","# @title Initialize lists to store data about sublinks\n","# Initialize lists to store data about sublinks:\n","pd_distance_RG_links_updated = pd_distance_RG_links.copy()\n","# print(pd_distance_RG_links)\n","# def get_d_closest_gauges(pd_distance_RG_links,sublink,d):\n","#   lst = list(pd_distance_RG_links.loc[sublink,:])\n","  # print(lst)\n","\n","# get_d_closest_gauges(pd_distance_RG_links,10,1)\n","def get_m_lowest_values_with_names(df, sublink, M):\n","    # Filter the DataFrame by the given sublink\n","    # print(\"sublink\", sublink)\n","    # print(df)\n","    filtered_df = df[df['Sublink'] == sublink]\n","\n","    if filtered_df.empty:\n","        return f\"No data found for Sublink {sublink}\"\n","\n","    # Drop the Sublink column\n","    filtered_df = filtered_df.drop(columns=['Sublink'])\n","\n","    # Get the M lowest values and their column names\n","    lowest_values = filtered_df.apply(lambda x: x.nsmallest(M)).dropna().stack().nsmallest(M)\n","\n","    return list(zip(lowest_values.index.get_level_values(1), lowest_values.values))\n","\n","\n","# Example usage\n","sublink = 195\n","M = 3\n","\n","# print(get_m_lowest_values_with_names(pd_distance_RG_links, sublink, 3))\n","d_close_gauges_names = get_m_lowest_values_with_names(pd_distance_RG_links, sublink, 3)"],"metadata":{"id":"kG9SNM_oEnWY","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"BPB52h5KKN6J"}},{"cell_type":"code","source":["# @title get rain gauges from gauges name\n","\n","print(raw_gauges)\n","def get_gauges_name(pd_distance_RG_links,sub_link,d):\n","  names_gauges = []\n","  d_close_gauges_names = get_m_lowest_values_with_names(pd_distance_RG_links, sublink, d)\n","  for elem in d_close_gauges_names:\n","    names_gauges.append(elem[0])\n","    return names_gauges\n","\n","def calculate_average(df, column_names):\n","    # Ensure the column names are in a list\n","    # if isinstance(column_names, str):\n","    #     column_names = [name.strip() for name in column_names.split(\",\")]\n","    # else:\n","    #     column_names = [name.strip() for name in column_names]\n","    names = []\n","    for elem in column_names:\n","      names = column_names[0]\n","    column_names = names\n","    # Filter the DataFrame to include only the specified columns and Time_UTC\n","    filtered_df = df[['Time_UTC'] + column_names]\n","    # Calculate the average for each of the specified columns\n","    average_df = filtered_df.copy()\n","    for column in column_names:\n","        average_df[column] = filtered_df[column].mean()\n","\n","    # Return the resulting DataFrame\n","    return average_df\n","\n","# d = 3\n","# sublink = 195\n","\n","# a = calculate_average(raw_gauges,get_gauges_name(pd_distance_RG_links, 195, 3))\n","# print(a)"],"metadata":{"id":"LaVd0q2IKRjB","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title rain_classification_from gauges\n","def rain_classification_from_gauges(raw_gauges,gauge_name):\n","  # print(\"raw_gauges\")\n","  # print(raw_gauges)\n","  raw_gauges_class_from_gauges = raw_gauges.copy()\n","  raw_gauges_class_from_gauges.iloc[:,1:-1] = raw_gauges.iloc[:,1:-1].applymap(lambda x : 1 if x>0 else 0)\n","  raw_gauges_class_from_gauges = raw_gauges_class_from_gauges.loc[:,[\"Time\",gauge_name]]\n","  #print(raw_gauges_class_from_gauges[10000:11100])\n","  #new_column_values = list(range(0, 7948870, 10))  # Generates [0, 10, 20, ..., 100]\n","  # Add this list as a new column to your DataFrame\n","  #class_rain_per_time['time_binary'] = new_column_values\n","  #print(raw_gauges_class_from_gauges)\n","  #plot graph:\n","  # plt.figure(figsize=(100, 5))  # Adjust figure size as needed\n","  ##plt.plot(class_rain_per_time['time_binary'], class_rain_per_time['rsl_164'], marker='o', linestyle='-', color='b', label='rsl_164')\n","  # plt.title('Plot of rsl_164 vs. time_binary')\n","  # plt.xlabel('time_binary')\n","  # plt.ylabel('rsl_164')\n","  # plt.grid(True)\n","  # plt.legend()\n","  # plt.tight_layout()\n","\n","  # # Show plot\n","  # plt.show()\n","  return raw_gauges_class_from_gauges\n","\n","\n","rain_classification_from_gauges1 = rain_classification_from_gauges(raw_gauges, \"Jarn\")\n","print(rain_classification_from_gauges1)\n","print(len(rain_classification_from_gauges1))\n","raw_gauges_class_from_gauges = rain_classification_from_gauges(raw_gauges, \"Jarn\")\n","print(raw_gauges_class_from_gauges)\n","print(len(raw_gauges_class_from_gauges))\n"],"metadata":{"id":"SUlsrMYfFlDQ","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ROC for optimal threshold\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def min_max_position(lst):\n","    arr = np.array(lst)\n","    min_pos = np.argmin(arr) + 1\n","    print(f\"The best threshold is {min_pos-1}\")\n","    return min_pos\n","\n","def unaccuracy(classification_sublink, classification_from_gauge):\n","    if len(classification_sublink) != len(classification_from_gauge):\n","        print(\"Lengths do not match!\")\n","        return -1\n","\n","    false_alarm = sum(1 for i in range(len(classification_sublink)) if classification_sublink[i] == 1 and classification_from_gauge[i] == 0)\n","    miss_detects = sum(1 for i in range(len(classification_sublink)) if classification_sublink[i] == 0 and classification_from_gauge[i] == 1)\n","    total = len(classification_sublink)\n","\n","    print(\"FP Rate:\", false_alarm / total)\n","    print(\"FN Rate:\", miss_detects / total)\n","    print(\"Total error rate:\", (miss_detects + false_alarm) / total)\n","\n","def calc_false_alarm_by_miss_detects(classification_sublink, classification_from_gauge):\n","    if len(classification_sublink) != len(classification_from_gauge):\n","        print(\"Lengths do not match!\")\n","        return -1\n","\n","    false_alarm = sum(1 for i in range(len(classification_sublink)) if classification_sublink[i] == 1 and classification_from_gauge[i] == 0)\n","    miss_detects = sum(1 for i in range(len(classification_sublink)) if classification_sublink[i] == 0 and classification_from_gauge[i] == 1)\n","    tn = sum(1 for i in range(len(classification_sublink)) if classification_sublink[i] == 0 and classification_from_gauge[i] == 0)\n","    tp = sum(1 for i in range(len(classification_sublink)) if classification_sublink[i] == 1 and classification_from_gauge[i] == 1)\n","\n","    return [false_alarm / (false_alarm + tn), tp / (tp + miss_detects)]\n","\n","def plot1(sub_link, gauge_name, threshold_range):\n","    raw_gauges_class_from_gauges = rain_classification_from_gauges(raw_gauges, gauge_name)\n","    classification_from_gauge = list(raw_gauges_class_from_gauges.loc[:, gauge_name]).copy()\n","\n","    classification_data = {}\n","    for i in threshold_range:\n","        classification_data[i] = get_rain_classification_from_sub_link(df_rain_att, sub_link, i)\n","\n","    fpr_values = []\n","    tpr_values = []\n","\n","    for i in threshold_range:\n","        classification_sub_link = classification_data[i][1]  # Extract classification result\n","        fpr, tpr = calc_false_alarm_by_miss_detects(classification_sub_link, classification_from_gauge)\n","        fpr_values.append(fpr)\n","        tpr_values.append(tpr)\n","\n","    # Plot ROC Curve\n","    plt.plot(fpr_values, tpr_values, marker='o')\n","\n","    # Adding labels and title\n","    plt.xlabel('False Positive Rate (FPR)')\n","    plt.ylabel('True Positive Rate (TPR)')\n","    plt.title('ROC Curve')\n","\n","    # Displaying the plot\n","    plt.grid(True)\n","    plt.show()\n","\n","    return fpr_values, tpr_values\n","\n","# Define threshold values\n","threshold_list = [-50, 0 ,1 ,2 ,3, 3.5 , 4, 5, 5.5,  6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 20, 30, 50]\n","\n","# Run the function\n","fpr_thresh, tpr_thresh = plot1(332, \"Tole\", threshold_list)\n"],"metadata":{"id":"R221NZqlL1s2","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title STD classification\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import auc\n","def calc_sigma_and_mean_values_new(vector):\n","    # Initialize lists to store sigma (standard deviation) and mean values\n","    mean_values = []\n","    sigma_values = []\n","\n","    # Loop over the vector, computing for each 30 consecutive elements\n","    for i in range(0,len(vector) - 7, 6):\n","        subset = vector[i:i+30]\n","        mean_values.append(np.mean(subset))\n","        sigma_values.append(np.std(subset))\n","\n","    # Convert the lists to arrays\n","    mean_values = np.array(mean_values)\n","    sigma_values = np.array(sigma_values)\n","    return sigma_values, mean_values\n","# Your given setup\n","a = list(df_rain_att['rsl_336'])\n","classification_from_gauge = list(raw_gauges_class_from_gauges.loc[:, 'Tole']).copy()\n","raw_gauges_class_from_gauges = rain_classification_from_gauges(raw_gauges, 'Tole')\n","\n","# Calculate sigma and mean\n","sigma, mean = calc_sigma_and_mean_values_new(a)\n","\n","# Convert classification to a NumPy array\n","classification_from_gauge = np.array(classification_from_gauge)\n","print(  len(sigma))\n","print(len(classification_from_gauge))\n","# Ensure classification length matches sigma length before filtering\n","min_length = min(len(sigma), len(classification_from_gauge))\n","sigma = sigma[:min_length]\n","classification_from_gauge = classification_from_gauge[:min_length]\n","\n","# Mask to remove NaNs from sigma and corresponding gauge classifications\n","mask = ~np.isnan(sigma)\n","\n","# Apply mask\n","sigma_values = sigma[mask]\n","classification_from_gauge = classification_from_gauge[mask]\n","\n","print(\"Filtered data sizes:\")\n","print(f\"Sigma length: {len(sigma_values)}\")\n","print(f\"Gauge classification length: {len(classification_from_gauge)}\")\n","\n","\n","\n","\n","\n","def find_optimal_sigma_threshold(sigma_values, true_values, num_thresholds=300):\n","    # Define a range of threshold values\n","    thresholds = np.linspace(np.min(sigma_values), np.max(sigma_values), num_thresholds)\n","\n","    best_threshold = None\n","    min_error = float('inf')\n","\n","    fp_values = []\n","    fn_values = []\n","    error_values = []\n","    fpr_values = []\n","    tpr_values = []\n","\n","    for threshold in thresholds:\n","        # Classify as rain (1) if sigma >= threshold, otherwise no rain (0)\n","        predicted = (sigma_values >= threshold).astype(int)\n","\n","        # Calculate FP, FN, TP, TN\n","        fp = np.sum((predicted == 1) & (true_values == 0))\n","        fn = np.sum((predicted == 0) & (true_values == 1))\n","        tp = np.sum((predicted == 1) & (true_values == 1))\n","        tn = np.sum((predicted == 0) & (true_values == 0))\n","\n","        # Compute rates\n","        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n","        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n","\n","        # Compute FP^2 + FN^2 error\n","        error = (fp / len(true_values))**2 + (fn / len(true_values))**2\n","\n","        # Store values\n","        fp_values.append(fp / len(true_values))\n","        fn_values.append(fn / len(true_values))\n","        error_values.append(error)\n","        fpr_values.append(fpr)\n","        tpr_values.append(tpr)\n","\n","        # Update best threshold\n","        if error < min_error:\n","            min_error = error\n","            best_threshold = threshold\n","\n","    # Plot FP, FN, and error function\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(thresholds, fp_values, label=\"FP Rate\", linestyle=\"dashed\", color=\"red\")\n","    plt.plot(thresholds, fn_values, label=\"FN Rate\", linestyle=\"dashed\", color=\"blue\")\n","    plt.plot(thresholds, error_values, label=\"FP + FN\", color=\"black\", linewidth=2)\n","    plt.axvline(best_threshold, color=\"green\", linestyle=\"dotted\", label=f\"Optimal Threshold: {best_threshold:.4f}\")\n","\n","    plt.xlabel(\"Sigma Threshold\")\n","    plt.ylabel(\"Error Rates\")\n","    plt.title(\"Optimizing Sigma Threshold for Rain Detection\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(f\"Optimal Sigma Threshold: {best_threshold:.4f} (Min FP + FN = {min_error:.6f})\")\n","\n","    # Plot ROC curve\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr_values, tpr_values, marker='o', label=f'ROC Curve (AUC = {auc(fpr_values, tpr_values):.4f}')\n","    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier')\n","    plt.xlabel('False Positive Rate (FPR)')\n","    plt.ylabel('True Positive Rate (TPR)')\n","    plt.title('ROC Curve for Rain Detection')\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","\n","    print('resulted fp and fn:')\n","    print(fp_values)\n","    print(fn_values)\n","    return best_threshold, fpr_values, tpr_values\n","\n","# Run optimization\n","optimal_sigma_threshold, fpr_values_std, tpr_values_std = find_optimal_sigma_threshold(sigma_values, classification_from_gauge)\n"],"metadata":{"id":"qWGgvplBgIwA","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title STD updated\n","# Optimizing sigma threshold with updated output for best FP and FN minimizing FP^2 + FN^2\n","def find_optimal_sigma_threshold(sigma_values, true_values, num_thresholds=300):\n","    # Define a range of threshold values\n","    thresholds = np.linspace(np.min(sigma_values), np.max(sigma_values), num_thresholds)\n","\n","    best_threshold = None\n","    min_error = float('inf')\n","    best_fp = 0\n","    best_fn = 0\n","\n","    fp_values = []\n","    fn_values = []\n","    error_values = []\n","    fpr_values = []\n","    tpr_values = []\n","\n","    for threshold in thresholds:\n","        # Classify as rain (1) if sigma >= threshold, otherwise no rain (0)\n","        predicted = (sigma_values >= threshold).astype(int)\n","\n","        # Calculate FP, FN, TP, TN\n","        fp = np.sum((predicted == 1) & (true_values == 0))\n","        fn = np.sum((predicted == 0) & (true_values == 1))\n","        tp = np.sum((predicted == 1) & (true_values == 1))\n","        tn = np.sum((predicted == 0) & (true_values == 0))\n","\n","        # Compute rates\n","        fpr = fp / len(true_values)  # FP rate as fraction of total samples\n","        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n","\n","        # Compute FP^2 + FN^2 error\n","        error = (fp / len(true_values))**2 + (fn / len(true_values))**2\n","\n","        # Store values\n","        fp_values.append(fpr)\n","        fn_values.append(fn / len(true_values))  # FN rate as fraction of total samples\n","        error_values.append(error)\n","        fpr_values.append(fpr)\n","        tpr_values.append(tpr)\n","\n","        # Update best threshold if error is minimized\n","        if error < min_error:\n","            min_error = error\n","            best_threshold = threshold\n","            best_fp = fpr  # Store FP rate\n","            best_fn = fn / len(true_values)  # Store FN rate\n","\n","    # Plot FP, FN, and error function\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(thresholds, fp_values, label=\"FP Rate\", linestyle=\"dashed\", color=\"red\")\n","    plt.plot(thresholds, fn_values, label=\"FN Rate\", linestyle=\"dashed\", color=\"blue\")\n","    plt.plot(thresholds, error_values, label=\"FP + FN\", color=\"black\", linewidth=2)\n","    plt.axvline(best_threshold, color=\"green\", linestyle=\"dotted\", label=f\"Optimal Threshold: {best_threshold:.4f}\")\n","\n","    plt.xlabel(\"Sigma Threshold\")\n","    plt.ylabel(\"Error Rates\")\n","    plt.title(\"Optimizing Sigma Threshold for Rain Detection\")\n","    plt.legend()\n","    plt.show()\n","\n","    # Print the best FP and FN rates as percentages\n","    print(f\"Optimal Sigma Threshold: {best_threshold:.4f} (Min FP + FN = {min_error:.6f})\")\n","    print(f\"Best FP Rate: {best_fp * 100:.2f}% (False Positive Rate)\")\n","    print(f\"Best FN Rate: {best_fn * 100:.2f}% (False Negative Rate)\")\n","\n","    # Plot ROC curve\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr_values, tpr_values, marker='o', label=f'ROC Curve (AUC = {auc(fpr_values, tpr_values):.4f})')\n","    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier')\n","    plt.xlabel('False Positive Rate (FPR)')\n","    plt.ylabel('True Positive Rate (TPR)')\n","    plt.title('ROC Curve for Rain Detection')\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","    return best_threshold, fpr_values, tpr_values\n","\n","# Run optimization\n","a = list(df_rain_att['rsl_163'])\n","classification_from_gauge = list(raw_gauges_class_from_gauges.loc[:, 'Jarn']).copy()\n","raw_gauges_class_from_gauges = rain_classification_from_gauges(raw_gauges, 'Jarn')\n","\n","# Calculate sigma and mean\n","sigma, mean = calc_sigma_and_mean_values_new(a)\n","\n","# Convert classification to a NumPy array\n","classification_from_gauge = np.array(classification_from_gauge)\n","print(  len(sigma))\n","print(len(classification_from_gauge))\n","# Ensure classification length matches sigma length before filtering\n","min_length = min(len(sigma), len(classification_from_gauge))\n","sigma = sigma[:min_length]\n","classification_from_gauge = classification_from_gauge[:min_length]\n","\n","# Mask to remove NaNs from sigma and corresponding gauge classifications\n","mask = ~np.isnan(sigma)\n","\n","# Apply mask\n","sigma_values = sigma[mask]\n","classification_from_gauge = classification_from_gauge[mask]\n","\n","optimal_sigma_threshold, fpr_values_std, tpr_values_std = find_optimal_sigma_threshold(sigma_values, classification_from_gauge)\n"],"metadata":{"id":"OuAMTj4zgXQT","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Combined ROC on the same graph for comparison\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import auc\n","\n","def plot_roc_curve(fpr_values_std, tpr_values_std, fpr_thresh, tpr_thresh):\n","    \"\"\"\n","    Plots the ROC curve comparing the standard deviation-based method and threshold-based method.\n","\n","    Parameters:\n","    - fpr_values_std: List of false positive rates (FPR) for the std-based method.\n","    - tpr_values_std: List of true positive rates (TPR) for the std-based method.\n","    - fpr_thresh: List of false positive rates (FPR) for the threshold-based method.\n","    - tpr_thresh: List of true positive rates (TPR) for the threshold-based method.\n","    \"\"\"\n","    plt.figure(figsize=(8, 6))\n","\n","    # Plot standard deviation-based ROC\n","    plt.plot(fpr_values_std, tpr_values_std, marker='o', linestyle='-', color='blue', label=f\"Std-based ROC (AUC = {auc(fpr_values_std, tpr_values_std):.4f})\")\n","\n","    # Plot threshold-based ROC\n","    plt.plot(fpr_thresh, tpr_thresh, marker='s', linestyle='-', color='red', label=f\"Threshold-based ROC (AUC = {auc(fpr_thresh, tpr_thresh):.4f})\")\n","\n","    # Plot random classifier baseline\n","    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Random Classifier (AUC = 0.5)\")\n","\n","    # Labels and legend\n","    plt.xlabel(\"False Positive Rate (FPR)\")\n","    plt.ylabel(\"True Positive Rate (TPR)\")\n","    plt.title(\"ROC Comparison: Std-based vs. Threshold-based\")\n","    plt.legend()\n","    plt.grid()\n","\n","    # Show plot\n","    plt.show()\n","print('fpr values')\n","print(fpr_thresh)\n","print('tpr values')\n","print(tpr_thresh)\n","print('=====================')\n","print(fpr_values_std)\n","print(tpr_values_std)\n","\n","print(len(fpr_thresh), len(tpr_thresh))\n","print(len(fpr_values_std), len(tpr_values_std))\n","\n","plot_roc_curve(fpr_values_std, tpr_values_std, fpr_thresh, tpr_thresh)\n","\n"],"metadata":{"id":"MBGCqNKqFu5N","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title k means one minute\n"],"metadata":{"id":"AKKuEyWWs1be","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title k means and svm\n","import numpy as np\n","from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","def k_means(sigma, mean):\n","    # Combine sigma and mean into a single 2D array\n","    data = np.vstack((sigma, mean)).T\n","\n","    # Remove rows containing NaN\n","    data = data[~np.isnan(data).any(axis=1)]\n","    print(data)\n","    # Apply K-means clustering with k=2\n","    kmeans = KMeans(n_clusters=2, random_state=0, n_init=10).fit(data)\n","    labels = kmeans.labels_\n","\n","    # Determine which cluster is wet and which is dry\n","    wet_cluster = np.argmax([np.mean(data[labels == i][:, 0]) for i in range(2)])\n","    dry_cluster = 1 - wet_cluster\n","\n","    # Create the classification array\n","    classification = np.where(labels == wet_cluster, 1, 0)\n","\n","    # Print the cluster centers\n","    print(\"Cluster Centers (sigma, mean):\")\n","    print(kmeans.cluster_centers_)\n","\n","    # Plot the data points colored by cluster\n","    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='coolwarm', alpha=0.5, label='Data Points')\n","\n","    # Plot the centroids\n","    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n","                s=300, c='yellow', marker='X', edgecolor='black', label='Centroids')\n","\n","    plt.xlabel('Standard Deviation (sigma)')\n","    plt.ylabel('Mean')\n","    plt.title('K-Means Clustering: Data Points and Centroids')\n","    plt.legend()\n","    plt.show()\n","\n","    # Return the classification result for comparison later\n","    return classification\n","\n","def calc_sigma_and_mean_values(vector):\n","    # Initialize lists to store sigma (standard deviation) and mean values\n","    mean_values = []\n","    sigma_values = []\n","\n","    # Loop over the vector, computing for each 30 consecutive elements\n","    for i in range(0,len(vector) - 29,30):\n","        subset = vector[i:i+30]\n","        mean_values.append(np.mean(subset))\n","        sigma_values.append(np.std(subset))\n","\n","    # Convert the lists to arrays\n","    mean_values = np.array(mean_values)\n","    sigma_values = np.array(sigma_values)\n","    return sigma_values, mean_values\n","\n","def compress_classification(classification, group_size=6):\n","    # Compress the classification by grouping 6 elements and marking 1 if all are 1\n","    compressed = []\n","    for i in range(0, len(classification), group_size):\n","        group = classification[i:i + group_size]\n","        # print('/////')\n","        # print(\"group\")\n","        # print(group)\n","        # compressed.append(1 if np.all(group == 1.0) else 0)\n","        compressed.append(1 if sum(group)> 0 else 0)\n","    return np.array(compressed)\n","\n","def compare_classifications(predicted, true_values):\n","    print(len(predicted))\n","    print(len(true_values))\n","    # Calculate FP and FN\n","    fp = np.sum((predicted == 1) & (true_values == 0))\n","    fn = np.sum((predicted == 0) & (true_values == 1))\n","\n","    print(f\"False Positives (FP): {fp/len(predicted)}\")\n","    print(f\"False Negatives (FN): {fn/len(predicted)}\")\n","\n","# Example usage:\n","\n","# Assume df_rain_att and gauge_classification are defined\n","a = list(df_rain_att['rsl_14'])\n","#classification_from_gauge = list(raw_gauges_class_from_gauges.loc[:,'Tole']).copy()\n","classification_from_gauge = np.array(get_gauge_estimation(raw_gauges, 'Tole'))\n","raw_gauges_class_from_gauges = rain_classification_from_gauges(raw_gauges, 'Tole')\n","# Calculate sigma and mean\n","sigma, mean = calc_sigma_and_mean_values(a)\n","\n","# Plot sigma vs. mean\n","plt.scatter(sigma, mean, alpha=0.3)\n","plt.xlabel('Standard Deviation (sigma)')\n","plt.ylabel('Mean')\n","plt.title('Scatter Plot of Sigma and Mean')\n","plt.show()\n","\n","# Apply K-means\n","classification = k_means(sigma, mean)\n","print(\"==\")\n","print(len(a))\n","print(len(classification))\n","print(len(classification_from_gauge))\n","# Compress the K-means result to match the size of gauge_classification\n","compressed_classification = compress_classification(classification_from_gauge, group_size=5)\n","print(\"===\")\n","print(len(classification))\n","print(len(compressed_classification))\n","classification = classification[:26000]\n","compressed_classification = compressed_classification[:26000]\n","# Compare with true values in gauge_classification\n","compare_classifications(classification, compressed_classification)\n","classification_k_means = classification\n","\n","# raw_gauges_class_from_gauges\n","\n","\n","# import numpy as np\n","# from sklearn.cluster import KMeans\n","# from sklearn.svm import SVC\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.metrics import classification_report\n","# from sklearn.preprocessing import StandardScaler\n","# import matplotlib.pyplot as plt\n","\n","# def k_means(sigma, mean):\n","#     # Combine sigma and mean into a single 2D array\n","#     data = np.vstack((sigma, mean)).T\n","\n","#     # Remove rows containing NaN\n","#     data = data[~np.isnan(data).any(axis=1)]\n","\n","#     # Apply K-means clustering with k=2\n","#     kmeans = KMeans(n_clusters=2, random_state=0, n_init=10).fit(data)\n","#     labels = kmeans.labels_\n","\n","#     # Determine which cluster is wet and which is dry\n","#     wet_cluster = np.argmax([np.mean(data[labels == i][:, 0]) for i in range(2)])\n","#     dry_cluster = 1 - wet_cluster\n","\n","#     # Create the classification array\n","#     classification = np.where(labels == wet_cluster, 1, 0)\n","\n","#     # Print the cluster centers\n","#     print(\"Cluster Centers (sigma, mean):\")\n","#     print(kmeans.cluster_centers_)\n","\n","#     # Return data and classification\n","#     return data, classification\n","\n","# def svm_classification(data, classification):\n","#     # Standardize the data\n","#     scaler = StandardScaler()\n","#     data_scaled = scaler.fit_transform(data)\n","\n","#     # Split the data into 75% training and 25% testing\n","#     X_train, X_test, y_train, y_test = train_test_split(data_scaled, classification, test_size=0.25, random_state=42)\n","\n","#     # Initialize the SVM classifier with an RBF kernel\n","#     svm_classifier = SVC(kernel='rbf', gamma='scale', C=1.0)\n","\n","#     # Train the SVM model\n","#     svm_classifier.fit(X_train, y_train)\n","\n","#     # Predict on test data\n","#     y_pred = svm_classifier.predict(X_test)\n","\n","#     # Print the classification report\n","#     print(\"SVM Classification Report:\")\n","#     print(classification_report(y_test, y_pred))\n","\n","#     # Visualize the decision boundary\n","#     plot_decision_boundary(X_train, y_train, svm_classifier, scaler)\n","\n","# def plot_decision_boundary(X, y, model, scaler):\n","#     # Create a mesh grid for plotting\n","#     h = 0.01\n","#     x_min, x_max = scaler.inverse_transform(X)[:, 0].min() - 1, scaler.inverse_transform(X)[:, 0].max() + 1\n","#     y_min, y_max = scaler.inverse_transform(X)[:, 1].min() - 1, scaler.inverse_transform(X)[:, 1].max() + 1\n","#     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","#                          np.arange(y_min, y_max, h))\n","\n","#     Z = model.predict(scaler.transform(np.c_[xx.ravel(), yy.ravel()]))\n","#     Z = Z.reshape(xx.shape)\n","\n","#     plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.8)\n","#     plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n","#     plt.xlabel('Standard Deviation (sigma)')\n","#     plt.ylabel('Mean')\n","#     plt.title('SVM Decision Boundary')\n","#     plt.show()\n","\n","# def calc_sigma_and_mean_values(vector):\n","#     # Initialize lists to store sigma (standard deviation) and mean values\n","#     mean_values = []\n","#     sigma_values = []\n","\n","#     # Loop over the vector, computing for each 30 consecutive elements\n","#     for i in range(len(vector) - 29):\n","#         subset = vector[i:i+30]\n","#         mean_values.append(np.mean(subset))\n","#         sigma_values.append(np.std(subset))\n","\n","#     # Convert the lists to arrays\n","#     mean_values = np.array(mean_values)\n","#     sigma_values = np.array(sigma_values)\n","#     return sigma_values, mean_values\n","\n","# Example usage with your data vector\n","print('#@%@@')\n","a = list(df_rain_att['rsl_14'])  # Adjust this to your actual data\n","sigma, mean = calc_sigma_and_mean_values(a)\n","print(len(a))\n","\n","# # Plot the sigma vs. mean scatter plot\n","# plt.scatter(sigma, mean, alpha=0.3)\n","# plt.xlabel('Standard Deviation (sigma)')\n","# plt.ylabel('Mean')\n","# plt.title('Scatter Plot of Sigma and Mean')\n","# plt.show()\n","\n","# # Perform K-means and get the classification labels\n","# data, classification = k_means(sigma, mean)\n","\n","# # Perform SVM classification using the K-means labels\n","# svm_classification(data, classification)\n"],"metadata":{"id":"eLSdvqpEWtF-","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 3-means\n","import numpy as np\n","from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","\n","def k_means(sigma, mean):\n","    # Combine sigma and mean into a single 2D array\n","    data = np.vstack((sigma, mean)).T\n","\n","    # Remove rows containing NaN\n","    data = data[~np.isnan(data).any(axis=1)]\n","\n","    # Apply K-means clustering with k=3\n","    kmeans = KMeans(n_clusters=3, random_state=0, n_init=10).fit(data)\n","    labels = kmeans.labels_\n","\n","    # Determine which clusters are wet, semi-wet, and dry based on sigma means\n","    cluster_means = [np.mean(data[labels == i][:, 0]) for i in range(3)]\n","    wet_cluster = np.argmax(cluster_means)  # Highest mean sigma\n","    dry_cluster = np.argmin(cluster_means)  # Lowest mean sigma\n","    semi_wet_cluster = 3 - wet_cluster - dry_cluster  # The remaining cluster\n","\n","    # Create the classification array: 2 for wet, 1 for semi-wet, 0 for dry\n","    classification = np.zeros_like(labels)\n","    classification[labels == wet_cluster] = 2\n","    classification[labels == semi_wet_cluster] = 1\n","    classification[labels == dry_cluster] = 0\n","\n","    # Print the cluster centers\n","    print(\"Cluster Centers (sigma, mean):\")\n","    print(kmeans.cluster_centers_)\n","\n","    # Plot the data points colored by cluster\n","    plt.scatter(data[:, 0], data[:, 1], c=classification, cmap='coolwarm', alpha=0.5, label='Data Points')\n","\n","    # Plot the centroids\n","    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n","                s=300, c='yellow', marker='X', edgecolor='black', label='Centroids')\n","\n","    plt.xlabel('Standard Deviation (sigma)')\n","    plt.ylabel('Mean')\n","    plt.title('K-Means Clustering: Data Points and Centroids')\n","    plt.legend()\n","    plt.show()\n","\n","    # Return the classification result for comparison later\n","    return classification\n","\n","def calc_sigma_and_mean_values(vector):\n","    # Initialize lists to store sigma (standard deviation) and mean values\n","    mean_values = []\n","    sigma_values = []\n","\n","    # Loop over the vector, computing for each 30 consecutive elements\n","    for i in range(0, len(vector) - 29, 30):\n","        subset = vector[i:i+30]\n","        mean_values.append(np.mean(subset))\n","        sigma_values.append(np.std(subset))\n","\n","    # Convert the lists to arrays\n","    mean_values = np.array(mean_values)\n","    sigma_values = np.array(sigma_values)\n","    return sigma_values, mean_values\n","\n","def compress_classification(classification, group_size=6):\n","    # Ensure classification is a NumPy array for element-wise operations\n","    classification = np.array(classification)\n","\n","    # Compress the classification by grouping 6 elements and marking 1 if any element is 1 or 2\n","    compressed = []\n","    for i in range(0, len(classification), group_size):\n","        group = classification[i:i + group_size]\n","        compressed.append(1 if np.any(group > 0) else 0)\n","    return np.array(compressed)\n","\n","def compare_classifications(predicted, true_values):\n","    # Convert \"wet\" (2) and \"semi-wet\" (1) classifications to 1, and \"dry\" (0) to 0\n","    binary_predicted = np.where(predicted > 0, 1, 0)\n","\n","    print(\"Predicted Classification Length:\", len(binary_predicted))\n","    print(\"True Classification Length:\", len(true_values))\n","\n","    # Calculate False Positives (FP) and False Negatives (FN)\n","    fp = np.sum((binary_predicted == 1) & (true_values == 0))\n","    fn = np.sum((binary_predicted == 0) & (true_values == 1))\n","\n","    print(f\"False Positives (FP): {fp / len(binary_predicted)}\")\n","    print(f\"False Negatives (FN): {fn / len(binary_predicted)}\")\n","\n","# Example usage:\n","\n","# Assume df_rain_att and gauge_classification are defined\n","a = list(df_rain_att['rsl_14'])\n","#classification_from_gauge = np.array(list(raw_gauges_class_from_gauges.loc[:, 'Tole']).copy())\n","classification_from_gauge = np.array(get_gauge_estimation(raw_gauges, 'Tole'))\n","\n","# Calculate sigma and mean\n","sigma, mean = calc_sigma_and_mean_values(a)\n","\n","# Plot sigma vs. mean\n","plt.scatter(sigma, mean, alpha=0.3)\n","plt.xlabel('Standard Deviation (sigma)')\n","plt.ylabel('Mean')\n","plt.title('Scatter Plot of Sigma and Mean')\n","plt.show()\n","\n","# Apply K-means with k=3\n","classification = k_means(sigma, mean)\n","\n","# Compress the gauge classification to match the size of K-means result\n","compressed_classification = compress_classification(classification_from_gauge, group_size=5)\n","\n","# Trim the classifications to match lengths for comparison\n","classification = classification[:26000]\n","compressed_classification = compressed_classification[:26000]\n","\n","# Compare with true values in gauge_classification\n","compare_classifications(classification, compressed_classification)\n"],"metadata":{"id":"u3EUKVrRHITn","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Fuzzy 2 Means\n","!pip install scikit-fuzzy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import skfuzzy as fuzz\n","\n","def fuzzy_c_means(sigma, mean):\n","    # Combine sigma and mean into a single 2D array\n","    data = np.vstack((sigma, mean))\n","\n","    # Remove rows containing NaN\n","    valid_idx = ~np.isnan(data).any(axis=0)\n","    data = data[:, valid_idx]\n","\n","    # Apply Fuzzy C-Means clustering with 2 clusters\n","    cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(\n","        data, c=2, m=2, error=0.005, maxiter=100000, init=None)\n","\n","    # Determine which cluster is 'wet' and which is 'dry'\n","    wet_cluster = np.argmax([np.mean(data[0, np.argmax(u, axis=0) == i]) for i in range(2)])\n","    dry_cluster = 1 - wet_cluster\n","\n","    # Membership probabilities for the 'wet' cluster\n","    wet_probabilities = u[wet_cluster]\n","\n","    # Plot data points colored by membership strength for the wet cluster\n","    plt.scatter(data[0], data[1], c=wet_probabilities, cmap='coolwarm', alpha=0.5)\n","    plt.colorbar(label='Membership Probability (Wet Cluster)')\n","    plt.scatter(cntr[:, 0], cntr[:, 1], s=300, c='yellow', marker='X', edgecolor='black', label='Centroids')\n","    plt.xlabel('Standard Deviation (sigma)')\n","    plt.ylabel('Mean')\n","    plt.title('Fuzzy C-Means Clustering: Data Points and Centroids')\n","    plt.legend()\n","    plt.show()\n","\n","    return wet_probabilities, valid_idx, data\n","\n","def calc_sigma_and_mean_values(vector):\n","    # Initialize lists to store sigma (standard deviation) and mean values\n","    mean_values = []\n","    sigma_values = []\n","\n","    # Loop over the vector, computing for each 30 consecutive elements\n","    for i in range(0, len(vector) - 29, 30):\n","        subset = vector[i:i+30]\n","        mean_values.append(np.mean(subset))\n","        sigma_values.append(np.std(subset))\n","\n","    # Convert the lists to arrays\n","    mean_values = np.array(mean_values)\n","    sigma_values = np.array(sigma_values)\n","    return sigma_values, mean_values\n","\n","def compress_classification(classification, group_size=5):\n","    # Compress the classification by grouping elements and marking 1 if any are 1\n","    compressed = []\n","    for i in range(0, len(classification), group_size):\n","        group = classification[i:i + group_size]\n","        compressed.append(1 if sum(group) > 0 else 0)\n","    return np.array(compressed)\n","\n","def calc_false_alarm_by_miss_detects_2(classification_sublink, classification_from_gauge):\n","    if(len(classification_sublink) != len(classification_from_gauge)):\n","        print(len(classification_sublink), len(classification_from_gauge))\n","        print(\"lens are don't match!\")\n","        return -1\n","    false_alarm = 0 #fp\n","    miss_detects = 0 #fn\n","    tn = 0 #tn\n","    tp = 0\n","    for i in range(len(classification_sublink)):\n","      if (classification_sublink[i] == 0  and classification_from_gauge[i] == 1):\n","        miss_detects +=1\n","      if (classification_sublink[i]== 1  and classification_from_gauge[i] == 0):\n","        false_alarm +=1\n","      if (classification_sublink[i] == 0  and classification_from_gauge[i] == 0):\n","        tn +=1\n","      if (classification_sublink[i] == 1  and classification_from_gauge[i] == 1):\n","        tp +=1\n","    l = len(classification_sublink)\n","    print([false_alarm/l , miss_detects/l ])\n","    # print((false_alarm/l)**2+2*(miss_detects/l)**2)\n","    # print((false_alarm)**2+2*(miss_detects)**2)\n","    # print((false_alarm)+2*(miss_detects))\n","    print(\"!!!!!!!!!!sfbjskfukVU\")\n","    print( 0.5 * false_alarm/(false_alarm+tn) +  miss_detects/(miss_detects+tp) )\n","\n","\n","def classify_and_plot_by_thresholds(wet_probabilities, valid_idx, data, classification_from_gauge):\n","    # Ensure wet_probabilities and valid_idx have the same length\n","    if len(wet_probabilities) != sum(valid_idx):\n","        # Trim wet_probabilities to match valid indices if necessary\n","        wet_probabilities = wet_probabilities[valid_idx]\n","\n","    thresholds = np.linspace(0.1, 1, 10)  # Generate several thresholds between 0 and 1\n","    for threshold in thresholds:\n","        # Classify points based on the current threshold\n","        classification = np.where(wet_probabilities >= threshold, 1, 0)\n","        print(classification)\n","        # Plot data points classified by the threshold\n","        plt.scatter(data[0, classification == 1], data[1, classification == 1], color='blue', label='Wet (1)', alpha=0.6)\n","        plt.scatter(data[0, classification == 0], data[1, classification == 0], color='red', label='Dry (0)', alpha=0.6)\n","        plt.xlabel('Standard Deviation (sigma)')\n","        plt.ylabel('Mean')\n","        plt.title(f'Classification with Threshold {threshold:.2f}')\n","        plt.legend()\n","        plt.show()\n","        print('===')\n","        calc_false_alarm_by_miss_detects_2(classification, classification_from_gauge)\n","\n","        print('===')\n","        # Compare with the gauge data if lengths match\n","        # if len(classification) == len(classification_from_gauge):\n","        #     # Calculate FP and FN counts and rates\n","        #     fp = np.sum((classification == 1.0) & (classification_from_gauge == 0.0))\n","        #     fn = np.sum((classification == 0.0) & (classification_from_gauge == 1.0))\n","        #     print('!!!')\n","        #     print(fn)\n","        #     fp_rate = fp / len(classification)\n","        #     fn_rate = fn / len(classification)\n","        #     print(f\"Threshold: {threshold:.2f}\")\n","        #     print(f\"False Positives (FP): {fp} (Rate: {fp_rate:.2%})\")\n","        #     print(f\"False Negatives (FN): {fn} (Rate: {fn_rate:.2%})\")\n","        #     print()\n","        # else:\n","        #     print(len(classification))\n","        #     print(len(classification_from_gauge))\n","        #     print(f\"Warning: Classification and gauge lengths differ for threshold {threshold:.2f}\")\n","\n","# Example usage:\n","\n","# Assume df_rain_att and gauge_classification are defined\n","\n","# print(raw_gauges_class_from_gauges)\n","# a = list(df_rain_att['rsl_332'])\n","# print(len(list(raw_gauges_class_from_gauges.loc[:, 'Tole']).copy()[:26000]))\n","# classification_from_gauge1 = list(raw_gauges_class_from_gauges.loc[:, 'Tole']).copy()[:26000]\n","\n","# # Calculate sigma and mean\n","# sigma, mean = calc_sigma_and_mean_values(a)\n","\n","# # Apply Fuzzy C-Means\n","# wet_probabilities, valid_idx, data = fuzzy_c_means(sigma[:26000], mean[:26000])\n","# fuzzy_probabilities_outputs = wet_probabilities.copy()\n","# # Compress the gauge data to match the size of the classification output\n","# #compressed_classification = compress_classification(classification_from_gaug1, group_size=5)\n","# #print(len(compressed_classification))\n","# print(len(classification_from_gauge1))\n","# # Classify and plot for various thresholds\n","# classify_and_plot_by_thresholds(wet_probabilities[:26000], valid_idx[:26000], data[:26000], classification_from_gauge1[:25856])\n"],"metadata":{"id":"bAK3WoOhtDFl","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Fuzzy 3 means\n","\n","import numpy as np\n","import skfuzzy as fuzz\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","\n","def fuzzy_c_means1(sigma, mean):\n","    # Apply logarithmic transformation to sigma and mean\n","    sigma = np.log1p(sigma)\n","    mean = np.log1p(mean)\n","\n","    # Combine sigma and mean into a single 2D array\n","    data = np.vstack((sigma, mean))\n","\n","    # Remove rows containing NaN\n","    valid_idx = ~np.isnan(data).any(axis=0)\n","    data = data[:, valid_idx]\n","\n","    # Normalize the data\n","    scaler = StandardScaler()\n","    data = scaler.fit_transform(data.T).T\n","\n","    # Apply Fuzzy C-Means clustering with 3 clusters\n","    num_clusters = 3\n","    cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(\n","        data, c=num_clusters, m=2, error=0.005, maxiter=100000, init=None\n","    )\n","\n","    # Determine cluster labels by comparing the mean of each cluster's sigma\n","    cluster_means = [np.mean(data[0, np.argmax(u, axis=0) == i]) for i in range(num_clusters)]\n","    sorted_clusters = np.argsort(cluster_means)  # Sort clusters by mean sigma\n","\n","    # Assign sorted clusters to meaningful labels\n","    cluster_probabilities = [u[sorted_clusters[i]] for i in range(num_clusters)]\n","\n","    # Find the first centroid (the one with the lowest mean and sigma)\n","    first_centroid = sorted_clusters[0]\n","\n","    # Get the membership probabilities for the first centroid (the 'rain' probability)\n","    rain_probabilities = u[first_centroid]\n","\n","    # Count how many points have a probability higher than 0.5 for the first centroid\n","    count_above_05_first_centroid = np.sum(rain_probabilities > 0.3)\n","\n","    # Calculate the percentage of points with probability > 0.5 for the first centroid\n","    total_points = data.shape[1]  # Total number of data points\n","    percentage_above_05 = (count_above_05_first_centroid / total_points) * 100\n","\n","    # Plot data points colored by membership strength for the first cluster\n","    plt.figure(figsize=(8, 6))\n","    plt.scatter(data[0], data[1], c=cluster_probabilities[0], cmap='coolwarm', alpha=0.5)\n","    plt.colorbar(label=f'Membership Probability (Cluster 1)')\n","    plt.scatter(cntr[:, 0], cntr[:, 1], s=300, c='yellow', marker='X', edgecolor='black', label='Centroids')\n","    plt.xlabel('Log Normalized Standard Deviation (sigma)')\n","    plt.ylabel('Log Normalized Mean')\n","    plt.title(f'Fuzzy C-Means Clustering: Cluster 1')\n","    plt.legend()\n","    #plt.show()\n","\n","    # Print the count and percentage of points with a membership probability greater than 0.5 for the first centroid\n","    print(f\"Count of points with probability > 0.5 for the first centroid (lowest sigma and mean): {count_above_05_first_centroid} points\")\n","    print(f\"Percentage of points with probability > 0.5 for the first centroid: {percentage_above_05:.2f}%\")\n","\n","    # Return rain probabilities, valid indices, and data\n","    return rain_probabilities, valid_idx, data\n","\n","\n","\n","\n","\n","\n","\n","# def fuzzy_c_means1(sigma, mean):\n","#     # Combine sigma and mean into a single 2D array\n","#     data = np.vstack((sigma, mean))\n","\n","#     # Remove rows containing NaN\n","#     valid_idx = ~np.isnan(data).any(axis=0)\n","#     data = data[:, valid_idx]\n","\n","#     # Apply Fuzzy C-Means clustering with 5 clusters\n","#     cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(\n","#         data, c=5, m=2, error=0.005, maxiter=100000, init=None)\n","\n","#     # Determine cluster labels by comparing the mean of each cluster's sigma\n","#     cluster_means = [np.mean(data[0, np.argmax(u, axis=0) == i]) for i in range(5)]\n","#     sorted_clusters = np.argsort(cluster_means)  # Sort clusters by mean sigma\n","\n","#     # Assign sorted clusters to meaningful labels (optional)\n","#     cluster_1, cluster_2, cluster_3, cluster_4, cluster_5 = sorted_clusters\n","\n","#     # Membership probabilities for each cluster\n","#     cluster_1_probabilities = u[cluster_1]\n","#     cluster_2_probabilities = u[cluster_2]\n","#     cluster_3_probabilities = u[cluster_3]\n","#     cluster_4_probabilities = u[cluster_4]\n","#     cluster_5_probabilities = u[cluster_5]\n","\n","#     # Plot data points colored by membership strength for each cluster (optional example: Cluster 1)\n","#     plt.scatter(data[0], data[1], c=cluster_5_probabilities, cmap='coolwarm', alpha=0.5)\n","#     plt.colorbar(label='Membership Probability (Cluster 5)')\n","#     plt.scatter(cntr[:, 0], cntr[:, 1], s=300, c='yellow', marker='X', edgecolor='black', label='Centroids')\n","#     plt.xlabel('Standard Deviation (sigma)')\n","#     plt.ylabel('Mean')\n","#     plt.title('Fuzzy C-Means Clustering: Data Points and Centroids')\n","#     plt.legend()\n","#     plt.show()\n","\n","#     # Return membership probabilities for all clusters, valid indices, and data\n","#     return (cluster_1_probabilities, cluster_2_probabilities, cluster_3_probabilities,\n","#             cluster_4_probabilities, cluster_5_probabilities, valid_idx, data)\n","\n","# Example usage (ensure `sigma` and `mean` are defined as inputs):\n","# sigma, mean = calc_sigma_and_mean_values(vector)\n","# cluster_probs = fuzzy_c_means(sigma, mean)\n","\n","\n","\n","# Assume df_rain_att and gauge_classification are defined\n","# a = list(df_rain_att['rsl_410'])\n","# print('!!!')\n","# print(raw_gauges_class_from_gauges)\n","# # print(len(list(raw_gauges_class_from_gauges.loc[:, 'Tole']).copy()[:26000]))\n","# # gauge_estimation = get_gauge_estimation(raw_gauges, gauge_name)[:132355]\n","# # classification_from_gauge1 = list(raw_gauges_class_from_gauges.loc[:, 'Tole']).copy()[:26000]\n","\n","\n","# # Calculate sigma and mean\n","# sigma, mean = calc_sigma_and_mean_values(a)\n","\n","# # Apply Fuzzy C-Means\n","# wet_probabilities, valid_idx, data = fuzzy_c_means1(sigma[:26000], mean[:26000])\n","# fuzzy_probabilities_outputs = wet_probabilities.copy()\n","# # Compress the gauge data to match the size of the classification output\n","# #compressed_classification = compress_classification(classification_from_gaug1, group_size=5)\n","# # #print(len(compressed_classification))\n","# # print(classification_from_gauge1)\n","# # Classify and plot for various thresholds\n","# # classify_and_plot_by_thresholds(wet_probabilities[:26000], valid_idx[:26000], data[:26000], classification_from_gauge1[:25856])\n","\n"],"metadata":{"id":"5h4ZgNk9alPk","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title r_power_law_functions\n","def r_power_law(att_ts , a, b , L ):\n","  # calculate rain rate from attenuation\n","  r_t =  (att_ts / (a*L))**(1/b)\n","  return r_t\n","\n","def r_power_law2(att_ts , a, b , L ):\n","  # calculate rain rate from attenuation\n","  r_t =  ((att_ts-0.15) / (a*L))**(1/b)\n","  return r_t\n","\n","def r_power_law3(att_ts , a, b , L ,dm):\n","  # calculate rain rate from attenuation\n","  r_t =  ((att_ts-dm) / (a*L))**(1/b)\n","  return r_t\n","def apply_r_power_law(df, col , a, b, L):\n","\n","    rain_pl = r_power_law(df[col], a, b, L)\n","    return rain_pl\n","\n","def apply_r_power_law2(df, col , a, b, L):\n","\n","    rain_pl = r_power_law2(df[col], a, b, L)\n","    return rain_pl\n","\n","def apply_r_power_law3(df, col , a, b, L, dm):\n","\n","    rain_pl = r_power_law3(df[col], a, b, L, dm)\n","    return rain_pl\n","\n","def apply_r_power_law_fix(df, col , a, b, L):\n","    print(\"a,b,l = :\")\n","    print(a)\n","    print(b)\n","    print(L)\n","    a1 = df[col]*2\n","    rain_pl = r_power_law(a1, a, b, L)\n","    return rain_pl\n","\n","# sublink_num = 10\n","# a_cml , b_cml = sublinks_ab[int(sublink_num)]\n","# L_cml = sublinks_L[int(sublink_num)]\n","# # ###\n","\n","# rain_mm = apply_r_power_law(df_rain_att, 'rsl_410' , a_cml, b_cml, L_cml)\n","# print(rain_mm)\n","# print(len(rain_mm))\n","# raw_col = raw_gauges['Chalm']\n","# print(raw_col)\n","# print(len(raw_col))\n","# 0.1595654"],"metadata":{"id":"GxNVqy-TbpPA","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Get gauge estimation by gauge name\n","def get_gauge_estimation(raw_gauges, gauge_name):\n","  raw_col = raw_gauges[gauge_name]\n","  return raw_col"],"metadata":{"id":"ElX2zr78t3h5","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Estimate rain amount from links according to power law\n","def get_rain_from_link(sublink_num):\n","  rsl = \"rsl_\" + str(sublink_num)\n","  a_cml , b_cml = sublinks_ab[int(sublink_num)]\n","  L_cml = sublinks_L[int(sublink_num)]\n","  rain_mm = np.array(apply_r_power_law(df_rain_att, rsl , a_cml, b_cml, L_cml))/360\n","  rain_mm_avg = []\n","  for i in range(len(rain_mm[:-1])//6+1):\n","    rain_mm_avg.append(np.sum(rain_mm[6*i:6*i+6]))\n","  return rain_mm_avg[2:]\n","\n","# Assuming 'raw_gauges' is a dataframe or dictionary with a \"Torp\" column/key\n","\n","# Assuming 'get_rain_from_link' returns the rain data\n","averaged_rain_mm_per_minute = get_rain_from_link(14)\n","# averaged_rain_mm_per_minute = get_rain_from_link(336)\n","# print(\"rain according to power law\")\n","# print(averaged_rain_mm_per_minute)\n","\n","# Convert the data to numpy arrays for stacking (assuming they are lists or pandas Series)\n","torp_values = np.array(raw_gauges[\"Tole\"])\n","rain_values = np.array(averaged_rain_mm_per_minute)\n","\n","# Horizontally stack the two time series using hstack (if needed for further processing)\n","stacked_data = np.hstack((torp_values.reshape(-1, 1), rain_values.reshape(-1, 1)))\n","\n","# Plot the two time series on the same plot\n","plt.figure(figsize=(10, 6))\n","\n","# Plotting the two series with different colors and labels\n","plt.plot(torp_values, label='Raw Gauges (Torp)', color='blue')\n","plt.plot(rain_values, label='Rain (Power Law)', linestyle='--', color='orange')\n","\n","# Adding labels and title\n","plt.xlabel('Time (index)')\n","plt.ylabel('Values')\n","plt.title('Comparison of Raw Gauges and Rain Data')\n","\n","# Adding grid for better visualization\n","plt.grid(True)\n","\n","# Adding a legend to describe the colors\n","plt.legend(loc='best')  # loc='best' automatically places the legend in the best location\n","\n","# Display the plot\n","plt.show()\n","\n","\n","\n","\n"],"metadata":{"id":"-SAd0Mt2DzjE","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Get Fuzzy detection by link and gauge\n","def get_fuzzy_by_link_and_gauge(df_rain_att, link, gauge):\n","  #  'rsl_336'\n","  a = list(df_rain_att[link])\n","  # print(len(list(raw_gauges_class_from_gauges.loc[:, 'Torp']).copy()[:26000]))\n","  classification_from_gauge = list(rain_classification_from_gauges(raw_gauges,gauge)).copy()[:26000]\n","\n","  # Calculate sigma and mean\n","  sigma, mean = calc_sigma_and_mean_values(a)\n","  # global fuzzy_probabilities_outputs\n","  # Apply Fuzzy C-Means\n","  wet_probabilities, valid_idx, data = fuzzy_c_means(sigma[:26000], mean[:26000])\n","  fuzzy_probabilities_outputs = wet_probabilities.copy()\n","  # Compress the gauge data to match the size of the classification output\n","  #compressed_classification = compress_classification(classification_from_gauge, group_size=5)\n","  #print(len(compressed_classification))\n","  # print(classification_from_gauge)\n","  # Classify and plot for various thresholds\n","\n","  # classify_and_plot_by_thresholds(wet_probabilities[:26000], valid_idx[:26000], data[:26000], classification_from_gauge[:25856])\n","\n","  return  fuzzy_probabilities_outputs\n","\n","\n","get_fuzzy_by_link_and_gauge(df_rain_att, 'rsl_14', 'Tole')"],"metadata":{"id":"eV6aIC34uH8g","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Estimation after Detection\n","def treshold_vector_to_detection(probabilities_fuzzy_detection,th_high, th_low):\n","   th_vector = probabilities_fuzzy_detection.copy()\n","   th_vector[th_vector > th_high] = 1\n","   th_vector[th_vector < th_low] = 0\n","   return th_vector\n","\n","\n","import numpy as np\n","\n","def calculate_rmse(A, B):\n","    # D = A\n","    # F = B\n","    # for i in range(len(D)):\n","    #   if(F[i] > 0.1):\n","    #     print('==')\n","    #     print('A[i], B[i]', D[i], F[i])\n","    # # Ensure the arrays are numpy arrays\n","    A = np.array(A)\n","    B = np.array(B)\n","\n","    # Check if the arrays have the same length\n","    if A.shape != B.shape:\n","        raise ValueError(\"Arrays must have the same length\")\n","\n","    # Remove NaN values from both arrays\n","    valid_mask = ~np.isnan(A) & ~np.isnan(B)\n","    # print('valid_mask', valid_mask)\n","    # print('valid', sum(valid_mask))\n","    A = A[valid_mask]\n","    B = B[valid_mask]\n","\n","    # Check if there are any valid data points left\n","    if len(A) == 0:\n","        raise ValueError(\"Arrays contain only NaN values\")\n","\n","    # Calculate the RMSE\n","    rmse = np.sqrt(np.sum((A - B) ** 2))\n","    # print('RMSE:', rmse)\n","    # rmse = np.sqrt(np.sum((A - B)**2))\n","    # print('RMSE:', rmse)\n","    return rmse\n","\n","\n","def compare_classifications(predicted, true_values):\n","    print(len(predicted))\n","    print(len(true_values))\n","    # Calculate FP and FN\n","    fp = np.sum((predicted == 1) & (true_values == 0))\n","    fn = np.sum((predicted == 0) & (true_values == 1))\n","\n","    # print(f\"False Positives (FP): {fp/len(predicted)}\")\n","    # print(f\"False Negatives (FN): {fn/len(predicted)}\")\n","\n","def get_key_with_min_value(my_dict):\n","    return min(my_dict, key=my_dict.get)\n","\n","\n","def unaccuracy(classification_sublink, classification_from_gauge):\n","  if(len(classification_sublink) != len(classification_from_gauge)):\n","      print(len(classification_sublink), len(classification_from_gauge))\n","      print(\"lens are don't match!\")\n","      return -1\n","  false_alarm = 0 #fp\n","  miss_detects = 0 #fn\n","  tn = 0 #tn\n","  for i in range(len(classification_sublink)):\n","    if (classification_sublink[i] == 0  and classification_from_gauge[i] == 1):\n","      miss_detects +=1\n","    if (classification_sublink[i]== 1  and classification_from_gauge[i] == 0):\n","      false_alarm +=1\n","    if (classification_sublink[i] == 0  and classification_from_gauge[i] == 0):\n","       tn +=1\n","  l = len(classification_sublink)\n","  # print(\"FP = \", false_alarm/l)\n","  # print(\"FN = \", miss_detects/l)\n","\n","\n","def chunk_average_array(arr, chunk_size=5):\n","    # Reshape the array into chunks of 5 (this only works if len(arr) is divisible by 5)\n","    reshaped = np.reshape(arr, (-1, chunk_size))\n","    # Calculate the average of each chunk\n","    new_array = np.mean(reshaped, axis=1)\n","    return new_array\n","\n","def compress_array_by_average(arr, chunk_size=5):\n","    # Ensure the length of the array is divisible by 5, or handle the remainder separately if needed\n","    trimmed_length = len(arr) - (len(arr) % chunk_size)\n","    arr = arr[:trimmed_length]  # Trim the array to be a multiple of chunk_size\n","\n","    # Reshape the array into chunks of 5 and calculate the mean of each chunk\n","    reshaped = np.reshape(arr, (-1, chunk_size))\n","    compressed_array = np.mean(reshaped, axis=1)\n","    return compressed_array\n","\n","def chunk_classification_array(arr, chunk_size=5):\n","    # Reshape the array into chunks of 5 (note: this only works if len(arr) is divisible by 5)\n","    reshaped = np.reshape(arr, (-1, chunk_size))\n","    # Take the maximum value in each chunk; if there's a 1, the chunk will return 1, otherwise 0\n","    new_array = np.max(reshaped, axis=1)\n","    return new_array\n","\n","def repeat_elements(arr, repeat_count=5):\n","    return np.repeat(arr, repeat_count)\n","\n","def compare_classifications(predicted, true_values):\n","    print(len(predicted))\n","    print(len(true_values))\n","    # Calculate FP and FN\n","    fp = np.sum((predicted == 1) & (true_values == 0))\n","    fn = np.sum((predicted == 0) & (true_values == 1))\n","\n","    # print(f\"False Positives (FP): {fp/len(predicted)}\")\n","    # print(f\"False Negatives (FN): {fn/len(predicted)}\")\n","\n","probabilities_fuzzy_detection = fuzzy_probabilities_outputs\n","# print(\"========\")\n","\n","classification_from_gauge = chunk_classification_array((list(raw_gauges_class_from_gauges.loc[:,'Torp']).copy()))\n","# print(\"lens1:\")\n","# print(len(probabilities_fuzzy_detection))\n","# print(len(classification_from_gauge))\n","# print(len(classification_from_gauge))\n","#[:25856]\n","#cutting:\n","probabilities_fuzzy_detection = probabilities_fuzzy_detection\n","#print(len(probabilities_fuzzy_detection))\n","classification_from_gauge = classification_from_gauge[:25856]\n","# print(\"nnn\")\n","# print(len(torp_values))\n","# #gauge_values = chunk_average_array(torp_values)\n","# print(\"mm\")\n","# print(len(gauge_values))\n","gauge_values = torp_values[:132355]\n","link_values = (rain_values)[2:132357]\n","# print('==')\n","# print(len(gauge_values))\n","# print(len(link_values))\n","# print(len(probabilities_fuzzy_detection))\n","\n","\n","gauge_values = compress_array_by_average(gauge_values)\n","link_values = compress_array_by_average(link_values)\n","# print('lens:')\n","# print(len(gauge_values))\n","# print(len(link_values))\n","# print(len(probabilities_fuzzy_detection))\n","\n","gauge_values = gauge_values[:25856]\n","link_values = link_values[:25856]\n","\n","#[0, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n","my_dict = {}\n","for th_high in [0, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]:\n","  for th_low in np.arange(0, th_high, 0.05) :\n","    class1 = treshold_vector_to_detection(probabilities_fuzzy_detection, th_high, th_low)[:25856]\n","    final_rain_estimation = class1*link_values\n","    my_dict[str(th_low)+','+str(th_high)] = calculate_rmse(final_rain_estimation, gauge_values)\n","    print( my_dict[str(th_low)+','+str(th_high)])\n","    # unaccuracy(class1, classification_from_gauge)\n","\n","#final result:\n","min_key = get_key_with_min_value(my_dict)\n","print('the results is')\n","print(min_key)  # Output will be 'b'\n","print(my_dict[min_key])\n","\n","\n"],"metadata":{"id":"aeUXd0mrHq7D","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title get Fuzzy 3 means according to link\n","def get_fuzzy_3_means_by_link_and_gauge(df_rain_att, link, gauge):\n","  #  'rsl_336'\n","  a = list(df_rain_att[link])\n","  # print(len(list(raw_gauges_class_from_gauges.loc[:, 'Torp']).copy()[:26000]))\n","  classification_from_gauge = list(rain_classification_from_gauges(raw_gauges,gauge)).copy()[:26000]\n","\n","  # Calculate sigma and mean\n","  sigma, mean = calc_sigma_and_mean_values(a)\n","  # global fuzzy_probabilities_outputs\n","  # Apply Fuzzy C-Means\n","  wet_probabilities, valid_idx, data = fuzzy_c_means1(sigma[:26000], mean[:26000])\n","  fuzzy_probabilities_outputs = wet_probabilities.copy()\n","  # Compress the gauge data to match the size of the classification output\n","  #compressed_classification = compress_classification(classification_from_gauge, group_size=5)\n","  #print(len(compressed_classification))\n","  # print(classification_from_gauge)\n","  # Classify and plot for various thresholds\n","\n","  # classify_and_plot_by_thresholds(wet_probabilities[:26000], valid_idx[:26000], data[:26000], classification_from_gauge[:25856])\n","\n","  return  fuzzy_probabilities_outputs\n"],"metadata":{"id":"6L6hFiNbmUmP","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Estimation After Detection examination for numbers of links\n","# # print(pd_distance_RG_links_updated)\n","# #[530, 336, 337, 411, 568, 567, 202]\n","# #define the desire link:\n","# # links_values = [14]\n","# #list_sublinks = [337, 202, 568, 333, 15, 600, 552, 530, 164]\n","# links_values = [336, 201, 567, 332, 14, 599, 551, 529, 163]\n","# #get the closest gauges names according to the links\n","# gauge_names = []\n","# for link in links_values:\n","#   gauge_names.append(get_m_lowest_values_with_names(pd_distance_RG_links_updated,link,1))\n","# gauge_names_updated = []\n","# for elem in gauge_names:\n","#   gauge_names_updated.append(elem[0][0])\n","\n","# print(gauge_names_updated)\n","# #here get the best thresholds\n","# link_counter = 0\n","# for gauge_name in gauge_names_updated:\n","\n","#   #  gauge_clasification =  chunk_classification_array((list(raw_gauges_class_from_gauges.loc[:,gauge_name]).copy()))[:25856]\n","#    gauge_estimation = get_gauge_estimation(raw_gauges, gauge_name)[:132355]\n","\n","#    link_name = 'rsl_'+str(links_values[link_counter])\n","#    print(f'Fuzzy output for {links_values[link_counter]}')\n","#   #  probabilities_fuzzy_detection = get_fuzzy_by_link_and_gauge(df_rain_att, link_name, gauge_name)[:25856]\n","#    probabilities_fuzzy_detection = get_fuzzy_3_means_by_link_and_gauge(df_rain_att, link_name, gauge_name)[:25856]\n","# #    link_values = (rain_values[2:])[:132355]\n","#    link_est = np.array(get_rain_from_link(links_values[link_counter]))[2:132357]\n","\n","#    gauge_estimation = compress_array_by_average(gauge_estimation)\n","#    link_est = compress_array_by_average(link_est)\n","#    gauge_estimation = gauge_estimation[:25856]\n","#    link_est = link_est[:25856]\n","\n","\n","#    #arrane the length of all the arrays:\n","#   #  print(len(probabilities_fuzzy_detection))\n","#   #  print(len(gauge_estimation))\n","#   #  print(len(link_est))\n","\n","#     # Plot the two time series on the same plot\n","#    plt.figure(figsize=(10, 6))\n","\n","#   # Plotting the two series with different colors and labels\n","#    plt.plot(link_est, label='link_est', color='blue')\n","#    plt.plot(gauge_estimation, label='gauge_estimation', linestyle='--', color='orange')\n","\n","#   # Adding labels and title\n","#    plt.xlabel('Time (index)')\n","#    plt.ylabel('Values')\n","#    plt.title('Comparison of link_est and gauge_estimation')\n","\n","#   # Adding grid for better visualization\n","#    plt.grid(True)\n","\n","#   # Adding a legend to describe the colors\n","#    plt.legend(loc='best')  # loc='best' automatically places the legend in the best location\n","\n","#   # Display the plot\n","#    plt.show()\n","\n","#    final_rain_estimation_keep = []\n","#    m_min = 500\n","#    my_dict = {}\n","#    for th_high in [0, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]:\n","#      for th_low in np.arange(0, th_high, 0.05) :\n","#        class1 = treshold_vector_to_detection(probabilities_fuzzy_detection, th_high, th_low)\n","#        final_rain_estimation = class1*link_est\n","#        m = calculate_rmse(final_rain_estimation, gauge_estimation)\n","#        my_dict[str(th_low)+','+str(th_high)] = m\n","#        if m<m_min:\n","#          m_min = m\n","#          final_rain_estimation_keep = final_rain_estimation\n","#    #final result:\n","#    print()\n","#    print('The best tresold for link -', links_values[link_counter])\n","#    min_key = get_key_with_min_value(my_dict)\n","#    print(min_key)\n","#    print(\"With RMSE value of\")\n","#    print(my_dict[min_key])\n","#    rmse = calculate_rmse(link_est, gauge_estimation)\n","#    print('Nive RMSE:', rmse)\n","\n","\n","\n","\n","#    print(\"\")\n","#   #  plt.figure(figsize=(10, 6))\n","\n","#   # # Plotting the two series with different colors and labels\n","#   #  plt.plot(final_rain_estimation_keep, label=' final_est', color='blue')\n","#   #  plt.plot(gauge_estimation, label='gauge_estimation', linestyle='--', color='orange')\n","\n","#   # # Adding labels and title\n","#   #  plt.xlabel('Time (index)')\n","#   #  plt.ylabel('Values')\n","#   #  plt.title('Comparison of final_est and gauge_estimation')\n","\n","#   # # Adding grid for better visualization\n","#   #  plt.grid(True)\n","\n","#   # # Adding a legend to describe the colors\n","#   #  plt.legend(loc='best')  # loc='best' automatically places the legend in the best location\n","\n","#   # # Display the plot\n","#   #  plt.show()\n","\n","\n","\n","#    plt.figure(figsize=(10, 6))\n","\n","#     # Plotting the two series with different styles\n","#    plt.plot(final_rain_estimation_keep, label='final_est', color='blue', linestyle='-', linewidth=2, marker='o', alpha=0.8)\n","#    plt.plot(gauge_estimation, label='gauge_estimation', linestyle='--', color='orange', linewidth=2, marker='s', alpha=0.8)\n","\n","#     # Adding labels and title\n","#    plt.xlabel('Time (index)')\n","#    plt.ylabel('Values')\n","#    plt.title('Comparison of final_est and gauge_estimation')\n","\n","#     # Adding grid for better visualization\n","#    plt.grid(True)\n","\n","#     # Adding a legend\n","#    plt.legend(loc='best')\n","\n","#     # Display the plot\n","#    plt.show()\n","\n","\n","\n","\n","\n","\n","#    best_final_rain_estimation = treshold_vector_to_detection(probabilities_fuzzy_detection, float(min_key.split(',')[1]), float(min_key.split(',')[0]))*link_est\n","#    # Plot the two time series on the same plot\n","#    plt.figure(figsize=(10, 6))\n","\n","#   # Plotting the two series with different colors and labels\n","#    plt.plot(probabilities_fuzzy_detection, label='Fuzzy', color='orange')\n","#    plt.plot(gauge_estimation, label='Gauge', linestyle='--', color='blue')\n","\n","#   # Adding labels and title\n","#    plt.xlabel('Time (index)')\n","#    plt.ylabel('Values')\n","#    plt.title('Comparison of Raw Gauges and probabilities_fuzzy_detection')\n","\n","#   # Adding grid for better visualization\n","#    plt.grid(True)\n","\n","#   # Adding a legend to describe the colors\n","#    plt.legend(loc='best')  # loc='best' automatically places the legend in the best location\n","\n","#   # Display the plot\n","#    plt.show()\n","\n","\n","#    link_counter +=1\n","\n","\n","\n"],"metadata":{"id":"dxdqPx1jwRhD","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Estimation After Detection examination for numbers of links FIX\n","# print(pd_distance_RG_links_updated)\n","#[530, 336, 337, 411, 568, 567, 202]\n","#define the desire link:\n","links_values = [14]\n","#get the closest gauges names according to the links\n","gauge_names = []\n","for link in links_values:\n","  gauge_names.append(get_m_lowest_values_with_names(pd_distance_RG_links_updated,link,1))\n","gauge_names_updated = []\n","for elem in gauge_names:\n","  gauge_names_updated.append(elem[0][0])\n","\n","print(gauge_names_updated)\n","#here get the best thresholds\n","link_counter = 0\n","for gauge_name in gauge_names_updated:\n","\n","  #  gauge_clasification =  chunk_classification_array((list(raw_gauges_class_from_gauges.loc[:,gauge_name]).copy()))[:25856]\n","   gauge_estimation = get_gauge_estimation(raw_gauges, gauge_name)\n","\n","   link_name = 'rsl_'+str(links_values[link_counter])\n","   print(f'Fuzzy output for {links_values[link_counter]}')\n","  #  probabilities_fuzzy_detection = get_fuzzy_by_link_and_gauge(df_rain_att, link_name, gauge_name)[:25856]\n","   probabilities_fuzzy_detection, m = get_fuzzy_3_means_by_link_and_gauge(df_rain_att, link_name, gauge_name)\n","#    link_values = (rain_values[2:])[:132355]\n","\n","\n","  #  link_est1 = np.array(get_rain_from_link(links_values[link_counter]))\n","   link_est = get_rain_from_link_and_db(links_values[link_counter], 0.1595654)\n","  #  link_est = get_rain_from_link_and_db(links_values[link_counter], 0)\n","   print('^^^^^^')\n","  #  print(len(link_est1))\n","  #  print(len(link_est2))\n","\n","   gauge_estimation = compress_array_by_average(gauge_estimation)\n","   link_est = compress_array_by_average(link_est)\n","   gauge_estimation = gauge_estimation\n","   link_est = link_est[m]\n","   print(len(gauge_estimation))\n","   print(len(link_est))\n","   print(len(compressed_classification))\n","   print(len(m))\n","   gauge_estimation = gauge_estimation[m]\n","\n","\n","\n","  #  gauge_estimation = compress_array_by_average(gauge_estimation)\n","  #  link_est = compress_array_by_average(link_est)\n","  #  gauge_estimation = gauge_estimation[:25856]\n","  #  link_est = link_est[:25856]\n","\n","\n","\n","    # Plot the two time series on the same plot\n","   plt.figure(figsize=(10, 6))\n","\n","  # Plotting the two series with different colors and labels\n","   plt.plot(link_est, label='link_est', color='blue')\n","   plt.plot(gauge_estimation, label='gauge_estimation', linestyle='--', color='orange')\n","\n","  # Adding labels and title\n","   plt.xlabel('Time (index)')\n","   plt.ylabel('Values')\n","   plt.title('Comparison of link_est and gauge_estimation')\n","\n","  # Adding grid for better visualization\n","   plt.grid(True)\n","\n","  # Adding a legend to describe the colors\n","   plt.legend(loc='best')  # loc='best' automatically places the legend in the best location\n","\n","  # Display the plot\n","   plt.show()\n","\n","   final_rain_estimation_keep = []\n","   m_min = 500\n","   my_dict = {}\n","   for th_high in [0, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]:\n","     for th_low in np.arange(0, th_high, 0.05) :\n","       class1 = treshold_vector_to_detection(probabilities_fuzzy_detection, th_high, th_low)\n","       final_rain_estimation = class1*link_est\n","       m = calculate_rmse(final_rain_estimation, gauge_estimation)\n","       my_dict[str(th_low)+','+str(th_high)] = m\n","       if m<m_min:\n","         m_min = m\n","         final_rain_estimation_keep = final_rain_estimation\n","   #final result:\n","   print()\n","   print('The best tresold for link -', links_values[link_counter])\n","   min_key = get_key_with_min_value(my_dict)\n","   print(min_key)\n","   print(\"With RMSE value of\")\n","   print(my_dict[min_key])\n","   rmse = calculate_rmse(link_est, gauge_estimation)\n","   print('Nive RMSE:', rmse)\n","\n","\n","\n","\n","   print(\"\")\n","  #  plt.figure(figsize=(10, 6))\n","\n","  # # Plotting the two series with different colors and labels\n","  #  plt.plot(final_rain_estimation_keep, label=' final_est', color='blue')\n","  #  plt.plot(gauge_estimation, label='gauge_estimation', linestyle='--', color='orange')\n","\n","  # # Adding labels and title\n","  #  plt.xlabel('Time (index)')\n","  #  plt.ylabel('Values')\n","  #  plt.title('Comparison of final_est and gauge_estimation')\n","\n","  # # Adding grid for better visualization\n","  #  plt.grid(True)\n","\n","  # # Adding a legend to describe the colors\n","  #  plt.legend(loc='best')  # loc='best' automatically places the legend in the best location\n","\n","  # # Display the plot\n","  #  plt.show()\n","\n","\n","\n","   plt.figure(figsize=(10, 6))\n","\n","    # Plotting the two series with different styles\n","   plt.plot(final_rain_estimation_keep, label='final_est', color='blue', linestyle='-', linewidth=2, marker='o', alpha=0.8)\n","   plt.plot(gauge_estimation, label='gauge_estimation', linestyle='--', color='orange', linewidth=2, marker='s', alpha=0.8)\n","\n","    # Adding labels and title\n","   plt.xlabel('Time (index)')\n","   plt.ylabel('Values')\n","   plt.title('Comparison of final_est and gauge_estimation')\n","\n","    # Adding grid for better visualization\n","   plt.grid(True)\n","\n","    # Adding a legend\n","   plt.legend(loc='best')\n","\n","    # Display the plot\n","   plt.show()\n","\n","\n","\n","\n","\n","\n","   best_final_rain_estimation = treshold_vector_to_detection(probabilities_fuzzy_detection, float(min_key.split(',')[1]), float(min_key.split(',')[0]))*link_est\n","   # Plot the two time series on the same plot\n","   plt.figure(figsize=(10, 6))\n","\n","  # Plotting the two series with different colors and labels\n","   plt.plot(probabilities_fuzzy_detection, label='Fuzzy', color='orange')\n","   plt.plot(gauge_estimation, label='Gauge', linestyle='--', color='blue')\n","\n","  # Adding labels and title\n","   plt.xlabel('Time (index)')\n","   plt.ylabel('Values')\n","   plt.title('Comparison of Raw Gauges and probabilities_fuzzy_detection')\n","\n","  # Adding grid for better visualization\n","   plt.grid(True)\n","\n","  # Adding a legend to describe the colors\n","   plt.legend(loc='best')  # loc='best' automatically places the legend in the best location\n","\n","  # Display the plot\n","   plt.show()\n","\n","\n","   link_counter +=1\n"],"metadata":{"id":"RLl48zkBD_0x","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title FUZZI ONE MINUTE\n","!pip install scikit-fuzzy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import skfuzzy as fuzz\n","\n","def fuzzy_c_means_1m(sigma, mean):\n","    # Combine sigma and mean into a single 2D array\n","    data = np.vstack((sigma, mean))\n","\n","    # Remove rows containing NaN\n","    valid_idx = ~np.isnan(data).any(axis=0)\n","    data = data[:, valid_idx]\n","\n","    # Apply Fuzzy C-Means clustering with 2 clusters\n","    cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(\n","        data, c=2, m=2, error=0.005, maxiter=100000, init=None)\n","\n","    # Determine which cluster is 'wet' and which is 'dry'\n","    wet_cluster = np.argmax([np.mean(data[0, np.argmax(u, axis=0) == i]) for i in range(2)])\n","    dry_cluster = 1 - wet_cluster\n","\n","    # Membership probabilities for the 'wet' cluster\n","    wet_probabilities = u[wet_cluster]\n","\n","    # Plot data points colored by membership strength for the wet cluster\n","    plt.scatter(data[0], data[1], c=wet_probabilities, cmap='coolwarm', alpha=0.5)\n","    plt.colorbar(label='Membership Probability (Wet Cluster)')\n","    plt.scatter(cntr[:, 0], cntr[:, 1], s=300, c='yellow', marker='X', edgecolor='black', label='Centroids')\n","    plt.xlabel('Standard Deviation (sigma)')\n","    plt.ylabel('Mean')\n","    plt.title('Fuzzy C-Means Clustering: Data Points and Centroids')\n","    plt.legend()\n","    plt.show()\n","\n","    return wet_probabilities, valid_idx, data\n","\n","def calc_sigma_and_mean_values(vector):\n","    # Initialize lists to store sigma (standard deviation) and mean values\n","    mean_values = []\n","    sigma_values = []\n","\n","    # Loop over the vector, computing for each 30 consecutive elements\n","    for i in range(0, len(vector) - 7, 6):\n","        subset = vector[i:i+6]\n","        mean_values.append(np.mean(subset))\n","        sigma_values.append(np.std(subset))\n","\n","    # Convert the lists to arrays\n","    mean_values = np.array(mean_values)\n","    sigma_values = np.array(sigma_values)\n","    return sigma_values, mean_values\n","\n","def compress_classification(classification, group_size=5):\n","    # Compress the classification by grouping elements and marking 1 if any are 1\n","    compressed = []\n","    for i in range(0, len(classification), group_size):\n","        group = classification[i:i + group_size]\n","        compressed.append(1 if sum(group) > 0 else 0)\n","    return np.array(compressed)\n","\n","def calc_false_alarm_by_miss_detects_2(classification_sublink, classification_from_gauge):\n","    if(len(classification_sublink) != len(classification_from_gauge)):\n","        print(\"lens are don't match!\")\n","        return -1\n","    false_alarm = 0 #fp\n","    miss_detects = 0 #fn\n","    tn = 0 #tn\n","    tp = 0\n","    for i in range(len(classification_sublink)):\n","      if (classification_sublink[i] == 0  and classification_from_gauge[i] == 1):\n","        miss_detects +=1\n","      if (classification_sublink[i]== 1  and classification_from_gauge[i] == 0):\n","        false_alarm +=1\n","      if (classification_sublink[i] == 0  and classification_from_gauge[i] == 0):\n","        tn +=1\n","      if (classification_sublink[i] == 1  and classification_from_gauge[i] == 1):\n","        tp +=1\n","    l = len(classification_sublink)\n","    print([false_alarm/l , miss_detects/l ])\n","    # print((false_alarm/l)**2+2*(miss_detects/l)**2)\n","    # print((false_alarm)**2+2*(miss_detects)**2)\n","    # print((false_alarm)+2*(miss_detects))\n","    print(\"!!!!!!!!!!sfbjskfukVU\")\n","    print((false_alarm/l)**2+2*(miss_detects/l)**2)\n","\n","\n","def classify_and_plot_by_thresholds(wet_probabilities, valid_idx, data, classification_from_gauge):\n","    # Ensure wet_probabilities and valid_idx have the same length\n","    if len(wet_probabilities) != sum(valid_idx):\n","        # Trim wet_probabilities to match valid indices if necessary\n","        wet_probabilities = wet_probabilities[valid_idx]\n","\n","    thresholds = np.linspace(0.1, 1, 10)  # Generate several thresholds between 0 and 1\n","    for threshold in thresholds:\n","        # Classify points based on the current threshold\n","        classification = np.where(wet_probabilities >= threshold, 1, 0)\n","        print(classification)\n","        # Plot data points classified by the threshold\n","        plt.scatter(data[0, classification == 1], data[1, classification == 1], color='blue', label='Wet (1)', alpha=0.6)\n","        plt.scatter(data[0, classification == 0], data[1, classification == 0], color='red', label='Dry (0)', alpha=0.6)\n","        plt.xlabel('Standard Deviation (sigma)')\n","        plt.ylabel('Mean')\n","        plt.title(f'Classification with Threshold {threshold:.2f}')\n","        plt.legend()\n","        plt.show()\n","        print('===')\n","        calc_false_alarm_by_miss_detects_2(classification,classification_from_gauge)\n","\n","        print('===')\n","        # Compare with the gauge data if lengths match\n","        # if len(classification) == len(classification_from_gauge):\n","        #     # Calculate FP and FN counts and rates\n","        #     fp = np.sum((classification == 1.0) & (classification_from_gauge == 0.0))\n","        #     fn = np.sum((classification == 0.0) & (classification_from_gauge == 1.0))\n","        #     print('!!!')\n","        #     print(fn)\n","        #     fp_rate = fp / len(classification)\n","        #     fn_rate = fn / len(classification)\n","        #     print(f\"Threshold: {threshold:.2f}\")\n","        #     print(f\"False Positives (FP): {fp} (Rate: {fp_rate:.2%})\")\n","        #     print(f\"False Negatives (FN): {fn} (Rate: {fn_rate:.2%})\")\n","        #     print()\n","        # else:\n","        #     print(len(classification))\n","        #     print(len(classification_from_gauge))\n","        #     print(f\"Warning: Classification and gauge lengths differ for threshold {threshold:.2f}\")\n","\n","# Example usage:\n","\n","# Assume df_rain_att and gauge_classification are defined\n","print('!!')\n","#print(raw_gauges_class_from_gauges)\n","a = list(df_rain_att['rsl_14'])\n","#print(len(list(raw_gauges_class_from_gauges.loc[:, 'Tole']).copy()[:26000]))\n","classification_from_gauge1 = list(get_gauge_estimation(raw_gauges, 'Tole')).copy()\n","sigma, mean = calc_sigma_and_mean_values(a)\n","data = np.vstack((sigma, mean)).T\n","mask = ~np.isnan(data).any(axis=1)\n","classification_from_gauge = np.array(classification_from_gauge1)\n","print(\"link length\")\n","print(len(a), len(mean))\n","print(\"class length and class from gauge length:\")\n","#print(len(classification))\n","print(len(classification_from_gauge))\n","print(\"check\")\n","print(mask.dtype)  # Should be 'bool'\n","print(mask.shape, classification_from_gauge.shape)\n","\n","data = data[~np.isnan(data).any(axis=1)]\n","classification_from_gauge1 = classification_from_gauge[mask]\n","\n","# Calculate sigma and mean\n","sigma, mean = calc_sigma_and_mean_values(a)\n","\n","# Apply Fuzzy C-Means\n","wet_probabilities, valid_idx, data = fuzzy_c_means_1m(sigma, mean)\n","fuzzy_probabilities_outputs = wet_probabilities.copy()\n","# Compress the gauge data to match the size of the classification output\n","#compressed_classification = compress_classification(classification_from_gaug1, group_size=5)\n","#print(len(compressed_classification))\n","print(classification_from_gauge1)\n","# Classify and plot for various thresholds\n","classify_and_plot_by_thresholds(wet_probabilities, valid_idx, data, classification_from_gauge1)\n"],"metadata":{"id":"p55gvI0HussB","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ROC FUZZY and compare\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import skfuzzy as fuzz\n","from sklearn.metrics import auc\n","\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import auc\n","\n","def plot_roc_curve_comparison(fpr_values_std, tpr_values_std, fpr_thresh, tpr_thresh, fpr_fuzzy, tpr_fuzzy):\n","    \"\"\"\n","    Plots the ROC curve comparing the standard deviation-based method, threshold-based method, and fuzzy clustering.\n","\n","    Parameters:\n","    - fpr_values_std: List of false positive rates (FPR) for the std-based method.\n","    - tpr_values_std: List of true positive rates (TPR) for the std-based method.\n","    - fpr_thresh: List of false positive rates (FPR) for the threshold-based method.\n","    - tpr_thresh: List of true positive rates (TPR) for the threshold-based method.\n","    - fpr_fuzzy: List of false positive rates (FPR) for the fuzzy clustering method.\n","    - tpr_fuzzy: List of true positive rates (TPR) for the fuzzy clustering method.\n","    \"\"\"\n","    plt.figure(figsize=(8, 6))\n","\n","    # Plot standard deviation-based ROC\n","    plt.plot(fpr_values_std, tpr_values_std, marker='o', linestyle='-', color='blue',\n","             label=f\"Std-based ROC (AUC = {auc(fpr_values_std, tpr_values_std):.4f})\")\n","\n","    # Plot threshold-based ROC\n","    plt.plot(fpr_thresh, tpr_thresh, marker='s', linestyle='-', color='red',\n","             label=f\"Threshold-based ROC (AUC = {auc(fpr_thresh, tpr_thresh):.4f})\")\n","\n","    # Plot fuzzy clustering-based ROC\n","    plt.plot(fpr_fuzzy, tpr_fuzzy, marker='^', linestyle='-', color='green',\n","             label=f\"Fuzzy-based ROC (AUC = {auc(fpr_fuzzy, tpr_fuzzy):.4f})\")\n","\n","    # Plot random classifier baseline\n","    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Random Classifier (AUC = 0.5)\")\n","\n","    # Labels and legend\n","    plt.xlabel(\"False Positive Rate (FPR)\")\n","    plt.ylabel(\"True Positive Rate (TPR)\")\n","    plt.title(\"ROC Comparison: Std-based vs. Threshold-based vs. Fuzzy-based\")\n","    plt.legend()\n","    plt.grid()\n","\n","    # Show plot\n","    plt.show()\n","\n","# Example call (ensure you have the respective FPR/TPR lists for each method)\n","# plot_roc_curve_comparison(fpr_values_std, tpr_values_std, fpr_thresh, tpr_thresh, fpr_fuzzy, tpr_fuzzy)\n","\n","\n","\n","def classify_and_plot_by_thresholds(wet_probabilities, valid_idx, data, classification_from_gauge):\n","    wet_probabilities = wet_probabilities[:valid_idx.sum()]  # Ensure matching dimensions\n","    #classification_from_gauge = classification_from_gauge[:valid_idx.sum()]  # Trim to match\n","\n","    thresholds = np.linspace(0, 1, 20)  # Ensure it starts from 0\n","    fpr_values, tpr_values = [], []\n","\n","    for threshold in thresholds:\n","        classification = (wet_probabilities >= threshold).astype(int)\n","\n","        print('classification')\n","        print(classification)\n","        print('classification_from_gauge')\n","        print(classification_from_gauge)\n","\n","        fp = np.sum((classification == 1) & (classification_from_gauge == 0))\n","        fn = np.sum((classification == 0) & (classification_from_gauge == 1))\n","        tp = np.sum((classification == 1) & (classification_from_gauge == 1))\n","        tn = np.sum((classification == 0) & (classification_from_gauge == 0))\n","\n","        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n","        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n","\n","        fpr_values.append(fpr)\n","        tpr_values.append(tpr)\n","\n","    auc_value = auc(fpr_values, tpr_values)\n","    print(f\"AUC: {auc_value:.4f}\")\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr_values, tpr_values, marker='o', label=f'ROC Curve (AUC = {auc_value:.4f})')\n","    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier')\n","    plt.xlabel('False Positive Rate (FPR)')\n","    plt.ylabel('True Positive Rate (TPR)')\n","    plt.title('ROC Curve for Rain Detection')\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","    return thresholds, fpr_values, tpr_values\n","classification_from_gauge = list(get_gauge_estimation(raw_gauges, 'Bergsj')).copy()\n","# Example usage:\n","wet_probabilities, valid_idx, data = fuzzy_c_means_1m(sigma, mean)\n","thresholds, fpr_fuzzy, tpr_fuzzy = classify_and_plot_by_thresholds(wet_probabilities, valid_idx, data, classification_from_gauge)\n","print('fpr, tpr values are:')\n","print(fpr_fuzzy)\n","print(tpr_fuzzy)\n","plot_roc_curve_comparison(fpr_values_std, tpr_values_std, fpr_thresh, tpr_thresh, fpr_fuzzy, tpr_fuzzy)\n"],"metadata":{"id":"Z4T9sqLivZET","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Default title text\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import auc\n","\n","def plot_roc_curve_comparison(fpr_values_std, tpr_values_std, fpr_thresh, tpr_thresh, fpr_fuzzy, tpr_fuzzy):\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr_values_std, tpr_values_std, marker='o', linestyle='-', color='blue',\n","             label=f\"Std-based ROC (AUC = {auc(fpr_values_std, tpr_values_std):.4f})\")\n","    plt.plot(fpr_thresh, tpr_thresh, marker='s', linestyle='-', color='red',\n","             label=f\"Threshold-based ROC (AUC = {auc(fpr_thresh, tpr_thresh):.4f})\")\n","    plt.plot(fpr_fuzzy, tpr_fuzzy, marker='^', linestyle='-', color='green',\n","             label=f\"Fuzzy-based ROC (AUC = {auc(fpr_fuzzy, tpr_fuzzy):.4f})\")\n","    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Random Classifier (AUC = 0.5)\")\n","    plt.xlabel(\"False Positive Rate (FPR)\")\n","    plt.ylabel(\"True Positive Rate (TPR)\")\n","    plt.title(\"ROC Comparison: Std-based vs. Threshold-based vs. Fuzzy-based\")\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","def classify_and_plot_by_thresholds(wet_probabilities, valid_idx, classification_from_gauge):\n","    wet_probabilities = wet_probabilities[:valid_idx.sum()]\n","\n","    # Convert classification_from_gauge to binary (0 or 1)\n","    classification_from_gauge = np.array(classification_from_gauge[:valid_idx.sum()]) >= 0.1\n","    classification_from_gauge = classification_from_gauge.astype(int)\n","\n","    thresholds = np.linspace(0, 1, 40)\n","    fpr_values, tpr_values = [], []\n","\n","    for threshold in thresholds:\n","        classification = (wet_probabilities >= threshold).astype(int)\n","\n","        fp = np.sum((classification == 1) & (classification_from_gauge == 0))\n","        fn = np.sum((classification == 0) & (classification_from_gauge == 1))\n","        tp = np.sum((classification == 1) & (classification_from_gauge == 1))\n","        tn = np.sum((classification == 0) & (classification_from_gauge == 0))\n","\n","        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n","        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n","\n","        fpr_values.append(fpr)\n","        tpr_values.append(tpr)\n","\n","    auc_value = auc(fpr_values, tpr_values)\n","    print(f\"AUC: {auc_value:.4f}\")\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr_values, tpr_values, marker='o', label=f'ROC Curve (AUC = {auc_value:.4f})')\n","    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier')\n","    plt.xlabel('False Positive Rate (FPR)')\n","    plt.ylabel('True Positive Rate (TPR)')\n","    plt.title('ROC Curve for Rain Detection')\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","    return thresholds, fpr_values, tpr_values\n","\n","# Example usage:\n","classification_from_gauge = list(get_gauge_estimation(raw_gauges, 'Jarn')).copy()\n","wet_probabilities, valid_idx, data = fuzzy_c_means_1m(sigma, mean)\n","thresholds, fpr_fuzzy, tpr_fuzzy = classify_and_plot_by_thresholds(wet_probabilities, valid_idx, classification_from_gauge)\n","\n","plot_roc_curve_comparison(fpr_values_std, tpr_values_std, fpr_thresh, tpr_thresh, fpr_fuzzy, tpr_fuzzy)\n"],"metadata":{"id":"ZKsCN-pYUsi6","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Dynamic Base line\n","import math\n","print(df_rain_att )#represent each mnute the attenuation\n","\n","def get_fuzzy_probabilities_and_mean_by_link(df_rain_att, link):\n","  a = list(df_rain_att[link])\n","  mean2 = np.array(a)\n","  print(\"!!!\")\n","  print(np.isnan(mean2).any())\n","  # classification_from_gauge = list(rain_classification_from_gauges(raw_gauges,gauge)).copy()[:26000]\n","  # Calculate sigma and mean\n","  # print('a', len(a))\n","  sigma, mean = calc_sigma_and_mean_values(a)\n","  # mean2 = np.array(mean)\n","  # print(\"!!!\")\n","  # print(np.isnan(mean2).any())\n","  # print('mean', len(mean))\n","  # global fuzzy_probabilities_outputs\n","  # Apply Fuzzy C-Means\n","  wet_probabilities, valid_idx, data = fuzzy_c_means(sigma[:26000], mean[:26000])\n","  fuzzy_probabilities_outputs = wet_probabilities.copy()\n","  # Compress the gauge data to match the size of the classification output\n","  #compressed_classification = compress_classification(classification_from_gauge, group_size=5)\n","  #print(len(compressed_classification))\n","  # print(classification_from_gauge)\n","  # Classify and plot for various thresholds\n","\n","  # classify_and_plot_by_thresholds(wet_probabilities[:26000], valid_idx[:26000], data[:26000], classification_from_gauge[:25856])\n","  clean_data = [x for x in mean if not (isinstance(x, float) and math.isnan(x))]\n","  return  fuzzy_probabilities_outputs, clean_data\n","\n","\n","def g(df_rain_att,link):\n","\n","  fuzzy_probabilties, mean = get_fuzzy_probabilities_and_mean_by_link(df_rain_att, link)\n","  # print('mean', len(mean))\n","  dict_classifiction = {}\n","  # print('===')\n","  # print(len(fuzzy_probabilties))\n","  # print(len(mean))\n","  # print('===')\n","  fuzzy_probabilties = fuzzy_probabilties[:25975]\n","  mean = mean[:25975]\n","  # mean2 = np.array(mean)\n","  # print(\"!!!\")\n","  # print(np.isnan(mean2).any())\n","  for thresh in [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]:\n","    classification = []\n","    list_dry = []\n","    for i in range(len(fuzzy_probabilties)):\n","      if fuzzy_probabilties[i] > thresh:\n","        classification.append(1)\n","      else:\n","        classification.append(0)\n","        list_dry.append(mean[i])\n","      # print(' list_dry = ',  list_dry)\n","      # if Nan in list_dry:\n","      #   print('nannnnn')\n","      if len(list_dry) != 0:\n","        avg_dry = sum(list_dry)/len(list_dry)\n","      else:\n","        avg_dry = 0\n","\n","    dict_classifiction[f'thresh_{thresh:.2f}'] = (classification, avg_dry)\n","\n","    print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n","    print(dict_classifiction[f'thresh_{thresh:.2f}'][1])\n","    # print('avg_dry =' , avg_dry)\n","  return dict_classifiction\n","\n","\n","a = g(df_rain_att, 'rsl_14')\n","\n","print('====')\n","# print(a)\n","\n","\n","\n","  # df_rain_att_update = compress_array_by_average(a) #chunk by 5 by avg\n","\n","\n","  #      list_dry"],"metadata":{"id":"_O2JCO2VJGti","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"klDiw0QKJGWr"}},{"cell_type":"code","source":["# @title change rain value according to db\n","# @title Estimate rain amount from links according to power law\n","def get_rain_from_link2(sublink_num):\n","  rsl = \"rsl_\" + str(sublink_num)\n","  a_cml , b_cml = sublinks_ab[int(sublink_num)]\n","  L_cml = sublinks_L[int(sublink_num)]\n","  rain_mm = np.array(apply_r_power_law(df_rain_att, rsl , a_cml, b_cml, L_cml))/360\n","  # rain_mm = np.array(apply_r_power_law2(df_rain_att, rsl , a_cml, b_cml, L_cml))/360\n","  rain_mm_avg = []\n","  for i in range(len(rain_mm[:-1])//6+1):\n","    rain_mm_avg.append(np.sum(rain_mm[6*i:6*i+6]))\n","  return rain_mm_avg[2:]\n","print(\"df_rain_att\")\n","print(df_rain_att)\n","# Assuming 'raw_gauges' is a dataframe or dictionary with a \"Torp\" column/key\n","print(\"raw gauges\")\n","print(raw_gauges[\"Chalm\"])\n","\n","# Assuming 'get_rain_from_link' returns the rain data\n","averaged_rain_mm_per_minute = get_rain_from_link2(14)\n","# averaged_rain_mm_per_minute = get_rain_from_link(336)\n","# print(\"rain according to power law\")\n","# print(averaged_rain_mm_per_minute)\n","\n","# Convert the data to numpy arrays for stacking (assuming they are lists or pandas Series)\n","torp_values = np.array(raw_gauges[\"Tole\"])\n","rain_values = np.array(averaged_rain_mm_per_minute)\n","print('!!!!')\n","print(rain_values)\n","# Horizontally stack the two time series using hstack (if needed for further processing)\n","stacked_data = np.hstack((torp_values.reshape(-1, 1), rain_values.reshape(-1, 1)))\n","\n","# Plot the two time series on the same plot\n","plt.figure(figsize=(10, 6))\n","\n","# Plotting the two series with different colors and labels\n","plt.plot(torp_values, label='Raw Gauges (Torp)', color='blue')\n","plt.plot(rain_values, label='Rain (Power Law)', linestyle='--', color='orange')\n","\n","# Adding labels and title\n","plt.xlabel('Time (index)')\n","plt.ylabel('Values')\n","plt.title('Comparison of Raw Gauges and Rain Data')\n","\n","# Adding grid for better visualization\n","plt.grid(True)\n","\n","# Adding a legend to describe the colors\n","plt.legend(loc='best')  # loc='best' automatically places the legend in the best location\n","\n","# Display the plot\n","plt.show()\n","\n","\n","\n","\n","\n"],"metadata":{"id":"eHTcWyz4SJ6q","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title given link and db return rain est\n","def get_rain_from_link_and_db(sublink_num, db):\n","  rsl = \"rsl_\" + str(sublink_num)\n","  a_cml , b_cml = sublinks_ab[int(sublink_num)]\n","  L_cml = sublinks_L[int(sublink_num)]\n","  rain_mm = np.array(apply_r_power_law3(df_rain_att, rsl , a_cml, b_cml, L_cml, db))/360\n","  rain_mm_avg = []\n","  for i in range(len(rain_mm[:-1])//6+1):\n","    rain_mm_avg.append(np.sum(rain_mm[6*i:6*i+6]))\n","  return np.array(rain_mm_avg[2:])\n","\n","\n","\n"],"metadata":{"cellView":"form","id":"6d5VciN-n3aI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title fuzzy with base line\n","# @title Estimation after Detection\n","def treshold_vector_to_detection(probabilities_fuzzy_detection,th_high, th_low):\n","   th_vector = probabilities_fuzzy_detection.copy()\n","   th_vector[th_vector > th_high] = 1\n","   th_vector[th_vector < th_low] = 0\n","   return th_vector\n","\n","\n","import numpy as np\n","\n","def calculate_rmse(A, B):\n","    # D = A\n","    # F = B\n","    # for i in range(len(D)):\n","    #   if(F[i] > 0.1):\n","    #     print('==')\n","    #     print('A[i], B[i]', D[i], F[i])\n","    # # Ensure the arrays are numpy arrays\n","    A = np.array(A)\n","    B = np.array(B)\n","\n","    # Check if the arrays have the same length\n","    if A.shape != B.shape:\n","        raise ValueError(\"Arrays must have the same length\")\n","\n","    # Remove NaN values from both arrays\n","    valid_mask = ~np.isnan(A) & ~np.isnan(B)\n","    # print('valid_mask', valid_mask)\n","    # print('valid', sum(valid_mask))\n","    A = A[valid_mask]\n","    B = B[valid_mask]\n","\n","    # Check if there are any valid data points left\n","    if len(A) == 0:\n","        raise ValueError(\"Arrays contain only NaN values\")\n","\n","    # Calculate the RMSE\n","    rmse = np.sqrt(np.sum((A - B) ** 2))\n","    print('RMSE:', rmse)\n","    # rmse = np.sqrt(np.sum((A - B)**2))\n","    # print('RMSE:', rmse)\n","    return rmse\n","\n","\n","def compare_classifications(predicted, true_values):\n","    print(len(predicted))\n","    print(len(true_values))\n","    # Calculate FP and FN\n","    fp = np.sum((predicted == 1) & (true_values == 0))\n","    fn = np.sum((predicted == 0) & (true_values == 1))\n","\n","    # print(f\"False Positives (FP): {fp/len(predicted)}\")\n","    # print(f\"False Negatives (FN): {fn/len(predicted)}\")\n","\n","def get_key_with_min_value(my_dict):\n","    return min(my_dict, key=my_dict.get)\n","\n","\n","def unaccuracy(classification_sublink, classification_from_gauge):\n","  if(len(classification_sublink) != len(classification_from_gauge)):\n","      print(len(classification_sublink), len(classification_from_gauge))\n","      print(\"lens are don't match!\")\n","      return -1\n","  false_alarm = 0 #fp\n","  miss_detects = 0 #fn\n","  tn = 0 #tn\n","  for i in range(len(classification_sublink)):\n","    if (classification_sublink[i] == 0  and classification_from_gauge[i] == 1):\n","      miss_detects +=1\n","    if (classification_sublink[i]== 1  and classification_from_gauge[i] == 0):\n","      false_alarm +=1\n","    if (classification_sublink[i] == 0  and classification_from_gauge[i] == 0):\n","       tn +=1\n","  l = len(classification_sublink)\n","  # print(\"FP = \", false_alarm/l)\n","  # print(\"FN = \", miss_detects/l)\n","\n","\n","def chunk_average_array(arr, chunk_size=5):\n","    # Reshape the array into chunks of 5 (this only works if len(arr) is divisible by 5)\n","    reshaped = np.reshape(arr, (-1, chunk_size))\n","    # Calculate the average of each chunk\n","    new_array = np.mean(reshaped, axis=1)\n","    return new_array\n","\n","def compress_array_by_average(arr, chunk_size=5):\n","    # Ensure the length of the array is divisible by 5, or handle the remainder separately if needed\n","    trimmed_length = len(arr) - (len(arr) % chunk_size)\n","    arr = arr[:trimmed_length]  # Trim the array to be a multiple of chunk_size\n","\n","    # Reshape the array into chunks of 5 and calculate the mean of each chunk\n","    reshaped = np.reshape(arr, (-1, chunk_size))\n","    compressed_array = np.mean(reshaped, axis=1)\n","    return compressed_array\n","\n","def chunk_classification_array(arr, chunk_size=5):\n","    # Reshape the array into chunks of 5 (note: this only works if len(arr) is divisible by 5)\n","    reshaped = np.reshape(arr, (-1, chunk_size))\n","    # Take the maximum value in each chunk; if there's a 1, the chunk will return 1, otherwise 0\n","    new_array = np.max(reshaped, axis=1)\n","    return new_array\n","\n","def repeat_elements(arr, repeat_count=5):\n","    return np.repeat(arr, repeat_count)\n","\n","def compare_classifications(predicted, true_values):\n","    print(len(predicted))\n","    print(len(true_values))\n","    # Calculate FP and FN\n","    fp = np.sum((predicted == 1) & (true_values == 0))\n","    fn = np.sum((predicted == 0) & (true_values == 1))\n","\n","    # print(f\"False Positives (FP): {fp/len(predicted)}\")\n","    # print(f\"False Negatives (FN): {fn/len(predicted)}\")\n","print(raw_gauges)\n","probabilities_fuzzy_detection = fuzzy_probabilities_outputs\n","print(\"========\")\n","print(raw_gauges_class_from_gauges)\n","# classification_from_gauge = chunk_classification_array((list(raw_gauges_class_from_gauges.loc[:,'Chalm']).copy()))\n","# print(\"lens1:\")\n","# print(len(probabilities_fuzzy_detection))\n","# print(len(classification_from_gauge))\n","# print(len(classification_from_gauge))\n","#[:25856]\n","#cutting:\n","probabilities_fuzzy_detection = probabilities_fuzzy_detection\n","#print(len(probabilities_fuzzy_detection))\n","classification_from_gauge = classification_from_gauge[:25856]\n","# print(\"nnn\")\n","# print(len(torp_values))\n","# #gauge_values = chunk_average_array(torp_values)\n","# print(\"mm\")\n","# print(len(gauge_values))\n","\n","\n","\n","#fuzzy db according to the thresholds: [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n","lst_db_fuzzy = [0.0, 0.12210448568312933, 0.13052027232158617, 0.1354149085126793, 0.13976774680964602, 0.14304621507153925, 0.1454825955644502,\n","  0.14905677774737558, 0.15147990379273463, 0.15467117769006078, 0.15789231470692736, 0.16025061063130053, 0.16620841436588335, 0.1726067078120952, 0.18154909662704585, 0.1892379192242681\n","                ,0.19773416347879047, 0.2095865435933632, 0.22350624313191395, 0.2393503425230597, 0.272532546442161]\n","\n","for db in lst_db_fuzzy:\n","    print('!!!!!!!db!!!!!!!!!!1\\n\\n\\n')\n","    print(db)\n","    rain_values = get_rain_from_link_and_db(14, db)\n","    torp_values = np.array(raw_gauges[\"Tole\"])\n","    # torp_values = np.array(raw_gauges[\"Torp\"])\n","    gauge_values = torp_values[:132355]\n","    link_values = (rain_values)[2:132357]\n","    # print('==')\n","    # print(len(gauge_values))\n","    # print(len(link_values))\n","    # print(len(probabilities_fuzzy_detection))\n","\n","\n","    gauge_values = compress_array_by_average(gauge_values)\n","    link_values = compress_array_by_average(link_values)\n","    # print('lens:')\n","    # print(len(gauge_values))\n","    # print(len(link_values))\n","    # print(len(probabilities_fuzzy_detection))\n","\n","    gauge_values = gauge_values[:25856]\n","    link_values = link_values[:25856]\n","    # print('keys')\n","    # print(a.keys())\n","    my_dict = {}\n","    for th_high in [0.00, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]:\n","      for th_low in np.arange(0.000, th_high, 0.05) :\n","        # print(th_low)\n","        class1 = treshold_vector_to_detection(probabilities_fuzzy_detection, th_high, th_low)[:25856]\n","        # final_rain_estimation = class1*(link_values+a[f'thresh_{th_low:.2f}'][1])\n","        final_rain_estimation = class1*(link_values)\n","        my_dict[str(th_low)+','+str(th_high)] = calculate_rmse(final_rain_estimation, gauge_values)\n","        # print( my_dict[str(th_low)+','+str(th_high)])\n","        # unaccuracy(class1, classification_from_gauge)\n","\n","    #final result:\n","    min_key = get_key_with_min_value(my_dict)\n","    print('the results is')\n","    print(min_key)\n","    print(my_dict[min_key])\n","\n","\n"],"metadata":{"id":"n_m4rxVP1-9_","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title helper\n","# @title get Fuzzy 3 means according to link\n","def get_fuzzy_3_means_by_link_and_gauge_up(df_rain_att, link, gauge):\n","  #  'rsl_336'\n","  a = list(df_rain_att[link])\n","  # print(len(list(raw_gauges_class_from_gauges.loc[:, 'Torp']).copy()[:26000]))\n","  classification_from_gauge = list(rain_classification_from_gauges(raw_gauges,gauge)).copy()[:26000]\n","\n","  # Calculate sigma and mean\n","  sigma, mean = calc_sigma_and_mean_values(a)\n","  # global fuzzy_probabilities_outputs\n","  # Apply Fuzzy C-Means\n","  wet_probabilities, valid_idx, data = fuzzy_c_means1(sigma[:26000], mean[:26000])\n","  fuzzy_probabilities_outputs = wet_probabilities.copy()\n","  # Compress the gauge data to match the size of the classification output\n","  #compressed_classification = compress_classification(classification_from_gauge, group_size=5)\n","  #print(len(compressed_classification))\n","  # print(classification_from_gauge)\n","  # Classify and plot for various thresholds\n","\n","  # classify_and_plot_by_thresholds(wet_probabilities[:26000], valid_idx[:26000], data[:26000], classification_from_gauge[:25856])\n","\n","  return  fuzzy_probabilities_outputs\n"],"metadata":{"cellView":"form","id":"40k1dsnhlx5R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title presentation\n","#define the desire link:\n","links_values = [332]\n","\n","\n","gauge_names = []\n","for link in links_values:\n","  gauge_names.append(get_m_lowest_values_with_names(pd_distance_RG_links_updated,link,1))\n","gauge_names_updated = []\n","for elem in gauge_names:\n","  gauge_names_updated.append(elem[0][0])\n","\n","print('closest gauge - ', gauge_names_updated)\n","#here get the best thresholds\n","link_counter = 0\n","for gauge_name in gauge_names_updated:\n","\n","  #  gauge_clasification =  chunk_classification_array((list(raw_gauges_class_from_gauges.loc[:,gauge_name]).copy()))[:25856]\n","   gauge_estimation = get_gauge_estimation(raw_gauges, gauge_name)[:132355]\n","\n","   link_name = 'rsl_'+str(links_values[link_counter])\n","   print(f'Fuzzy output for {links_values[link_counter]}')\n","  #  probabilities_fuzzy_detection = get_fuzzy_by_link_and_gauge(df_rain_att, link_name, gauge_name)[:25856]\n","   probabilities_fuzzy_detection = get_fuzzy_3_means_by_link_and_gauge_up(df_rain_att, link_name, gauge_name)[:25856]\n","#    link_values = (rain_values[2:])[:132355]\n","   link_est = np.array(get_rain_from_link(links_values[link_counter]))[2:132357]\n","\n","   gauge_estimation = compress_array_by_average(gauge_estimation)\n","   link_est = compress_array_by_average(link_est)\n","   gauge_estimation = gauge_estimation[:25856]\n","   link_est = link_est[:25856]\n","\n","\n","   #arrane the length of all the arrays:\n","  #  print(len(probabilities_fuzzy_detection))\n","  #  print(len(gauge_estimation))\n","  #  print(len(link_est))\n","\n","    # Plot the two time series on the same plot\n","   plt.figure(figsize=(10, 6))\n","\n","  # Plotting the two series with different colors and labels\n","   plt.plot(link_est, label='link_est', color='blue')\n","   plt.plot(gauge_estimation, label='gauge_estimation', linestyle='--', color='orange')\n","\n","  # Adding labels and title\n","   plt.xlabel('Time (index)')\n","   plt.ylabel('Values')\n","   plt.title('Comparison of link_est and gauge_estimation')\n","\n","  # Adding grid for better visualization\n","   plt.grid(True)\n","\n","  # Adding a legend to describe the colors\n","   plt.legend(loc='best')  # loc='best' automatically places the legend in the best location\n","\n","  # Display the plot\n","   plt.show()\n","\n","   final_rain_estimation_keep = []\n","   m_min = 500\n","   my_dict = {}\n","   for th_high in [0, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]:\n","     for th_low in np.arange(0, th_high, 0.05) :\n","       class1 = treshold_vector_to_detection(probabilities_fuzzy_detection, th_high, th_low)\n","       final_rain_estimation = class1*link_est\n","       m = calculate_rmse(final_rain_estimation, gauge_estimation)\n","       my_dict[str(th_low)+','+str(th_high)] = m\n","       if m<m_min:\n","         m_min = m\n","         final_rain_estimation_keep = final_rain_estimation\n","   #final result:\n","   print()\n","   print('The best tresold for link -', links_values[link_counter])\n","   min_key = get_key_with_min_value(my_dict)\n","   print(min_key)\n","   print(\"With RMSE value of\")\n","   print(my_dict[min_key])\n","   rmse = calculate_rmse(link_est, gauge_estimation)\n","   print('Nive RMSE:', rmse)\n","\n","\n","   plt.figure(figsize=(10, 6))\n","\n","    # Plotting the two series with different styles\n","   plt.plot(final_rain_estimation_keep, label='final_est', color='blue', linestyle='-', linewidth=2, marker='o', alpha=0.8)\n","   plt.plot(gauge_estimation, label='gauge_estimation', linestyle='--', color='orange', linewidth=2, marker='s', alpha=0.8)\n","\n","    # Adding labels and title\n","   plt.xlabel('Time (index)')\n","   plt.ylabel('Values')\n","   plt.title('Comparison of final_est and gauge_estimation')\n","\n","    # Adding grid for better visualization\n","   plt.grid(True)\n","\n","    # Adding a legend\n","   plt.legend(loc='best')\n","\n","    # Display the plot\n","   plt.show()\n","\n","\n","   best_final_rain_estimation = treshold_vector_to_detection(probabilities_fuzzy_detection, float(min_key.split(',')[1]), float(min_key.split(',')[0]))*link_est\n","   # Plot the two time series on the same plot\n","   plt.figure(figsize=(10, 6))\n","\n","  # Plotting the two series with different colors and labels\n","   plt.plot(probabilities_fuzzy_detection, label='Fuzzy', color='orange')\n","   plt.plot(gauge_estimation, label='Gauge', linestyle='--', color='blue')\n","\n","  # Adding labels and title\n","   plt.xlabel('Time (index)')\n","   plt.ylabel('Values')\n","   plt.title('Comparison of Raw Gauges and probabilities_fuzzy_detection')\n","\n","  # Adding grid for better visualization\n","   plt.grid(True)\n","\n","  # Adding a legend to describe the colors\n","   plt.legend(loc='best')  # loc='best' automatically places the legend in the best location\n","\n","  # Display the plot\n","   plt.show()\n","\n","\n","   link_counter +=1\n","\n","\n","\n"],"metadata":{"id":"QBFSKRTIfN21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1 Remove the Last Commit (Containing the PAT)\n","!git reset --soft HEAD~1  # Remove the last commit but keep changes locally\n","\n","# 2 Force Remount Google Drive (if needed)\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# 3 Open the notebook and remove the PAT from the file\n","# Make sure you open your notebook and remove the Personal Access Token (PAT) from the URL.\n","\n","# 4 Re-Add, Commit & Push\n","%cd /content/CML-Sensing\n","\n","# Add the notebook again (without PAT)\n","!git add CML_Sensing_Sweden_Final.ipynb\n","\n","# Commit the change (removed PAT from notebook)\n","!git commit -m \"Removed PAT from notebook\"\n","\n","# Set the remote URL with your GitHub username and Personal Access Token (PAT)\n","!git remote remove origin\n","!git remote add origin https://OrShmueli1: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@github.com/OrShmueli1/CML-Sensing.git\n","\n","# Rename branch to 'main' if needed\n","!git branch -M main\n","\n","# Push the commit to GitHub\n","!git push -u origin main\n"],"metadata":{"id":"bqESKGwSl4or"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SZCMD7vopPSh"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1sY5Yj9cuwIiCnbTL8sQKzF5dmqJwiFM-","timestamp":1740434182397},{"file_id":"1yZshC6rt0UxKTHaqyo3UwUksvPTmy1VU","timestamp":1740434117801}],"private_outputs":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}